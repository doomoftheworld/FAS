{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e88ded8",
   "metadata": {},
   "source": [
    "### *Module Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbfb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.display import display as ip_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbc82f",
   "metadata": {},
   "source": [
    "### *External Module Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_modules_path = '..\\\\nn_likelihood_modules'\n",
    "sys.path.append(external_modules_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_network_structure import *\n",
    "from common_imports import *\n",
    "from common_use_functions import *\n",
    "from constant import *\n",
    "from defined_data_structure import *\n",
    "from defined_network_structure import *\n",
    "from distribution_calculation import *\n",
    "from direct_inference_likelihood import *\n",
    "from experim_neural_network import *\n",
    "from experim_preparation import *\n",
    "from generate_activation_level import *\n",
    "from pytorch_model_predict import *\n",
    "from vector_preprocessing import *\n",
    "from ResNet import *\n",
    "from experim_ResNet import *\n",
    "from cifar_10_data_prep import *\n",
    "from novelty_data_prep import *\n",
    "from sensitivity_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc22ef",
   "metadata": {},
   "source": [
    "### *GPU verification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nb_gpu = torch.cuda.device_count()\n",
    "if nb_gpu > 0:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701537f",
   "metadata": {},
   "source": [
    "### *Working directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current path\n",
    "current_path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c8167",
   "metadata": {},
   "source": [
    "### *Load configurations and data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All the parameters in this part should be configured\n",
    "\"\"\"\n",
    "# Experience path\n",
    "experim_path = current_path\n",
    "\n",
    "# File extensions\n",
    "json_ext = '.json'\n",
    "np_ext = '.npy'\n",
    "csv_ext = '.csv'\n",
    "\n",
    "# ResNet model prefix\n",
    "model_name_prefix = 'cifar10'\n",
    "\n",
    "# Image max pixel value\n",
    "image_max_pix_val = 255\n",
    "\n",
    "# Tested sets name\n",
    "train_set_name = 'train'\n",
    "test_set_name = 'test'\n",
    "valid_set_name = 'valid'\n",
    "input_extension = 'X'\n",
    "label_extension = 'Y'\n",
    "\n",
    "# Save paths\n",
    "model_save_path = path_join(experim_path, 'experim_models_resnet')\n",
    "\n",
    "# Adversarial attack path\n",
    "adv_attack_path = path_join(experim_path, 'experim_resnet_attack')\n",
    "\n",
    "\"\"\"\n",
    "The following parameters should be configured according to your experiments\n",
    "\"\"\"\n",
    "\n",
    "# Trained model name \n",
    "trained_resnet_name = 'cifar10_resnet18_9304' # You can select any model from the \"experim_models_resnet\" folder\n",
    "\n",
    "# Indices filenames\n",
    "train_indices_filename = 'train_indices_9304' # The train indices should be coherent with your chosen model\n",
    "valid_indices_filename = 'valid_indices_9304' # The valid indices should be coherent with your chosen model\n",
    "\n",
    "# ResNet related params\n",
    "resnet_model_name = 'resnet18'# The model name should be coherent with your chosen model\n",
    "\n",
    "# Cifar10-c datafolder path\n",
    "cifar10_c_path = 'D:\\\\Doctorat\\\\research\\\\oms_detection_experim\\\\CIFAR-10-C\\CIFAR-10-C\\\\'\n",
    "\n",
    "# Dataset general informations\n",
    "data_set_infos = {\n",
    "    'nb_classes' : 10\n",
    "}\n",
    "\n",
    "# Distance decision filtering threshold\n",
    "std_threshold_coeff = 2\n",
    "\n",
    "# The method to determining the significant neurons (mean or most_common)\n",
    "sobol_filter_method = 'most_common'\n",
    "\n",
    "# Rscript launch params\n",
    "Rscript_path = 'C:\\\\Program Files\\\\R\\\\R-4.3.3\\\\bin\\\\Rscript.exe'\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# The output folder\n",
    "output_path = path_join(experim_path, 'output')\n",
    "\n",
    "# Distribution file\n",
    "train_distribution_filename = path_join(output_path, 'distribution_' + train_set_name + '_set.csv')\n",
    "\n",
    "# Train distance information registration path\n",
    "train_dists_info_filename = 'train_dist_infos'\n",
    "train_dists_info_path = path_join(output_path, train_dists_info_filename + json_ext)\n",
    "\n",
    "# Build the class list\n",
    "class_list = list(range(data_set_infos['nb_classes']))\n",
    "\n",
    "# Batch size for the dataloader creation\n",
    "torch_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "create_directory(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ae1fb",
   "metadata": {},
   "source": [
    "### *Experiment preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55636cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "cifar10_train_dataset, cifar10_test_dataset = get_cifar10_dataset_without_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names of the filtering results\n",
    "column_names_OOD_filtering = ['transformation', 'nb_examples', 'nb_OOD', 'nb_InD', 'total_acc', 'OOD_acc', 'InD_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9931900",
   "metadata": {},
   "source": [
    "### *Load the trained ResNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resnet\n",
    "trained_resnet = load_model_by_net_name(model_save_path, trained_resnet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849492d",
   "metadata": {},
   "source": [
    "### *Cifar10 dataset preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd46a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train valid indices\n",
    "train_indices = load_json(open(path_join(model_save_path, train_indices_filename+json_ext)))\n",
    "valid_indices = load_json(open(path_join(model_save_path, valid_indices_filename+json_ext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the subsets\n",
    "cifar10_real_train_dataset = Subset(cifar10_train_dataset, train_indices)\n",
    "cifar10_valid_dataset = Subset(cifar10_train_dataset, valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a90a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader building\n",
    "train_loader = create_loader_from_torch_dataset(cifar10_real_train_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "valid_loader = create_loader_from_torch_dataset(cifar10_valid_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = create_loader_from_torch_dataset(cifar10_test_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed01114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training set to numpy array\n",
    "no_divide_into_batch_train_loader = create_loader_from_torch_dataset(cifar10_real_train_dataset, batch_size=len(cifar10_real_train_dataset), shuffle=False, num_workers=0)\n",
    "cifar10_train_X = next(iter(no_divide_into_batch_train_loader))[0].numpy()\n",
    "cifar10_train_y = next(iter(no_divide_into_batch_train_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to numpy array\n",
    "no_divide_into_batch_test_loader = create_loader_from_torch_dataset(cifar10_test_dataset, batch_size=len(cifar10_test_dataset), shuffle=False, num_workers=0)\n",
    "X_test = next(iter(no_divide_into_batch_test_loader))[0].numpy()\n",
    "y_test = next(iter(no_divide_into_batch_test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9019d",
   "metadata": {},
   "source": [
    "### *Distribution and Training set likelihood distance information generation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ea3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to gpu\n",
    "trained_resnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de9ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction\n",
    "cifar10_train_y_pred = np.array(predict_gpu(trained_resnet, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec410613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correctly predicted entries\n",
    "bool_correct_examples = cifar10_train_y==cifar10_train_y_pred\n",
    "cifar10_X_train_correct = cifar10_train_X[bool_correct_examples]\n",
    "cifar10_y_train_correct = cifar10_train_y[bool_correct_examples]\n",
    "# Data Loader\n",
    "correct_train_loader = create_dataloader(cifar10_X_train_correct, cifar10_y_train_correct, torch_batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86131e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activation levels\n",
    "train_actLevels = obtain_activation_levels(trained_resnet,\n",
    "                                           correct_train_loader, train_set_name, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c371a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution generation\n",
    "# Generate the distribution\n",
    "train_distributions = generate_distributions(train_actLevels, train_set_name, trained_resnet_name)\n",
    "# Save the distribution\n",
    "train_distribution_filename = save_distributions(output_path, train_distributions, train_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51800577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the likelihood information per class on the training set\n",
    "# Last hidden layer index and activation levels\n",
    "last_hidden_layerId = max(train_actLevels['actLevel'].keys())\n",
    "# Calculate the likelihood on this layer\n",
    "train_last_hidden_whole_distances = layer_whole_likelihood_experim(train_actLevels, last_hidden_layerId,\n",
    "                               train_distribution_filename, class_list, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the information and save it\n",
    "# Information calculation\n",
    "class_dist_infos = build_layer_train_set_infos(train_last_hidden_whole_distances, class_list)\n",
    "# Register the calculated information\n",
    "store_dict_as_json(train_dists_info_path, class_dist_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b8bc8",
   "metadata": {},
   "source": [
    "### *Evaluate the likelihood on the whole training set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf27140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activation levels \n",
    "#(We named this all_train_actLevels because we have already the activation levels for the correctly predicted examples)\n",
    "all_train_actLevels = obtain_activation_levels(trained_resnet,\n",
    "                                           train_loader, 'whole train', with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e794da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood on the last hidden layer\n",
    "all_train_whole_distances = layer_whole_likelihood_experim(all_train_actLevels, last_hidden_layerId,\n",
    "                               train_distribution_filename, class_list, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the whole distances\n",
    "all_train_whole_distances = normalize_whole_distances(all_train_whole_distances, class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bdbb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapped layer distances dataframe\n",
    "all_train_distances = map_to_predicted_class_distance(all_train_whole_distances, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83faf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decisions to be filtered\n",
    "all_train_filtered_index = filter_decision_based_on_train_infos_norm_ver(all_train_whole_distances, class_dist_infos, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the filtered training set examples\n",
    "if len(all_train_filtered_index) != 0:\n",
    "    ip_display(all_train_distances.loc[all_train_filtered_index,:])\n",
    "else:\n",
    "    print('All the example images are identified as good inputs by the likelihood distance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb8c6c",
   "metadata": {},
   "source": [
    "### *Evaluate the likelihood on the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2001aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activation levels\n",
    "test_actLevels = obtain_activation_levels(trained_resnet,\n",
    "                                           test_loader, 'test', with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood on this layer\n",
    "test_whole_distances = layer_whole_likelihood_experim(test_actLevels, last_hidden_layerId,\n",
    "                               train_distribution_filename, class_list, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "test_whole_distances = normalize_whole_distances(test_whole_distances, class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906cf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decisions to be filtered\n",
    "test_filtered_index = filter_decision_based_on_train_infos_norm_ver(test_whole_distances, class_dist_infos, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9051073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapped layer distances dataframe\n",
    "test_distances = map_to_predicted_class_distance(test_whole_distances, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce126bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the filtered test examples\n",
    "if len(test_filtered_index) != 0:\n",
    "    ip_display(test_distances.loc[test_filtered_index,:])\n",
    "else:\n",
    "    print('All the example images are identified as good inputs by the likelihood distance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd61ec",
   "metadata": {},
   "source": [
    "### *Sobol index evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correctly predicted index\n",
    "correctly_predicted_bools = all_train_actLevels['class'] == all_train_actLevels['predict_class']\n",
    "# Get the corresponding data\n",
    "last_hidden_actLevels = all_train_actLevels['actLevel'][last_hidden_layerId][correctly_predicted_bools.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last layer parameters\n",
    "model_params = get_model_parameters(trained_resnet, to_numpy=True)\n",
    "final_linear_params = model_params['linear_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b055e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of weight and input\n",
    "# Copy the original activation levels and weights\n",
    "normalized_last_hidden_actLevel = copy.deepcopy(last_hidden_actLevels)\n",
    "upped_final_linear_params = copy.deepcopy(final_linear_params)\n",
    "# Check the min and max values of each neuron\n",
    "last_hidden_actLevel_max = np.max(last_hidden_actLevels, axis=0)\n",
    "last_hidden_actLevel_min = np.min(last_hidden_actLevels, axis=0) # Not used, just for verification\n",
    "# Iterate over the maximum values and normalize the input\n",
    "for index, neuron_max in enumerate(last_hidden_actLevel_max):\n",
    "    if neuron_max != 0:\n",
    "        normalized_last_hidden_actLevel[:,index] = normalized_last_hidden_actLevel[:,index] / neuron_max\n",
    "        upped_final_linear_params['weight'][:,index] = upped_final_linear_params['weight'][:,index] * neuron_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f70433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final linear parameters per class and assign the real data\n",
    "final_linear_param_per_class = build_per_class_linear_model(upped_final_linear_params)\n",
    "data = normalized_last_hidden_actLevel\n",
    "# Number of variables\n",
    "nb_vars = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the X and y for the sobol index evaluation in R\n",
    "# Get the neuron names\n",
    "neuron_names = ['neuron_'+str(index) for index in range(last_hidden_actLevels.shape[1])]\n",
    "# Build the X dataframe\n",
    "R_X = pd.DataFrame(data, columns=neuron_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673887af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the y for the sobol index evaluation in R\n",
    "# Generate the y values\n",
    "R_y_values_per_class = {}\n",
    "for classId in final_linear_param_per_class:\n",
    "    current_class_y_values = []\n",
    "    for _, one_x in R_X.iterrows():\n",
    "        current_class_y_values.append(evaluate_y(final_linear_param_per_class[classId], one_x))\n",
    "    R_y_values_per_class[classId] = current_class_y_values\n",
    "# Build the y dataframes\n",
    "R_y_per_class = {}\n",
    "for classId in R_y_values_per_class:\n",
    "    R_y_per_class[classId] = pd.DataFrame(R_y_values_per_class[classId], columns=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X and y for R script execution\n",
    "save_df_to_csv(path_join(output_path, 'R_X.csv'),R_X)\n",
    "for classId in R_y_per_class:\n",
    "    save_df_to_csv(path_join(output_path, 'R_y_'+str(classId)+'.csv'),R_y_per_class[classId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd985f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataframe that stores weights and bias\n",
    "R_network_params_data = []\n",
    "R_network_params_columns = ['classId', *(['weight_'+str(index) for index in range(len(final_linear_param_per_class[0]['weight']))]), 'bias']\n",
    "for classId in final_linear_param_per_class:\n",
    "    R_network_params_data.append([classId, *list(final_linear_param_per_class[classId]['weight']), final_linear_param_per_class[classId]['bias']])\n",
    "R_network_params = pd.DataFrame(R_network_params_data, columns=R_network_params_columns)\n",
    "save_df_to_csv(path_join(output_path, 'R_network_params.csv'),R_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b47a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N = 22000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random samples from R_X\n",
    "A_index = generate_sample_index(data, N, replace=False)\n",
    "B_index = generate_sample_index_exclude_items(data, N, A_index, replace=False)\n",
    "R_X_A = R_X.loc[A_index,:].copy(deep=True).reset_index(drop=True)\n",
    "R_X_B = R_X.loc[B_index,:].copy(deep=True).reset_index(drop=True)\n",
    "# Save the random samples from R_X\n",
    "save_df_to_csv(path_join(output_path, 'R_X_A.csv'),R_X_A)\n",
    "save_df_to_csv(path_join(output_path, 'R_X_B.csv'),R_X_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could choose sobolEff (first and total indices)\n",
    "R_sobol_method = 'sobolEff'\n",
    "R_sobol_script = path_join(experim_path, R_sobol_method+'_eval.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the R script (The path to read the data is given as arguments (i.e., experim_path+'\\\\'))\n",
    "ouput_R = run([Rscript_path, '--vanilla', R_sobol_script, output_path+'\\\\', str(data_set_infos['nb_classes'])], shell=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the important variables per class\n",
    "# Initialize the determined variables as None\n",
    "important_variables_per_class = None\n",
    "if R_sobol_method == 'sobolEff':\n",
    "    # Load the sobol indices (first and total) and convert it to a dictionary\n",
    "    R_first_order_sobol_per_class_dict = {}\n",
    "    R_total_order_sobol_per_class_dict = {}\n",
    "    for classId in class_list:\n",
    "        R_current_class_first_order_sobol_indices = read_csv_to_pd_df(path_join(output_path, R_sobol_method+'_fs_'+str(classId)+csv_ext))\n",
    "        R_current_class_total_order_sobol_indices = read_csv_to_pd_df(path_join(output_path, R_sobol_method+'_tt_'+str(classId)+csv_ext))\n",
    "        R_first_order_sobol_per_class_dict[classId] = R_current_class_first_order_sobol_indices['S.original'].to_dict()\n",
    "        R_total_order_sobol_per_class_dict[classId] = R_current_class_total_order_sobol_indices['S.original'].to_dict()\n",
    "    # Get the important variables per class\n",
    "    important_variables_per_class = important_variables_R_first_and_total_order_analysis(R_first_order_sobol_per_class_dict, \n",
    "                                                                                         R_total_order_sobol_per_class_dict,\n",
    "                                                                                         class_list, filter_method=sobol_filter_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d060fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataframe that contains the number of neurons\n",
    "determined_nb_important_vars_per_class = []\n",
    "for classId in important_variables_per_class:\n",
    "    class_nb_important_vars = len(list(important_variables_per_class[classId].keys()))\n",
    "    determined_nb_important_vars_per_class.append([classId, nb_vars, class_nb_important_vars])\n",
    "determined_nb_important_vars_per_class_df = pd.DataFrame(determined_nb_important_vars_per_class, columns=['classId', 'nb_neurons', 'nb_important_neurons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e621ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the important variables\n",
    "store_dict_as_json(path_join(output_path, trained_resnet_name+'_important_neurons.json'), important_variables_per_class)\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_nb_important_neurons.csv'), determined_nb_important_vars_per_class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57224d4f",
   "metadata": {},
   "source": [
    "### *Likelihood calculation based only on the important variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sorted total important variable(neuron) indices\n",
    "sorted_important_var_by_class = {}\n",
    "for classId in important_variables_per_class:\n",
    "    sorted_important_var_by_class[classId] = sorted(list(important_variables_per_class[classId]))\n",
    "# Build the mapping dictionary to modify neuron indices\n",
    "important_var_map_dict_by_class = {}\n",
    "for classId in sorted_important_var_by_class:\n",
    "    important_var_map_dict_by_class[classId] = build_map_to_index_dict(sorted_important_var_by_class[classId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f1e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the activation levels of the important variables for each class\n",
    "built_train_actLevels_by_class = build_actLevel_important_vars(all_train_actLevels, sorted_important_var_by_class, last_hidden_layerId)\n",
    "built_test_actLevels_by_class = build_actLevel_important_vars(test_actLevels, sorted_important_var_by_class, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb71999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the training set distribution only for the important neurons\n",
    "# Read the training set distribution\n",
    "train_whole_distribution = pd.read_csv(train_distribution_filename, sep=' ')\n",
    "# Take the distribution for different classes\n",
    "train_important_var_distrib_by_class = {}\n",
    "for classId in sorted_important_var_by_class:\n",
    "    # Get the important variables of the current class\n",
    "    current_important_neuron_indices = sorted_important_var_by_class[classId]\n",
    "    # Take only the last hidden layer distribution (We calculate the likelihood only based on this)\n",
    "    train_whole_last_hidden_distribution = train_whole_distribution[train_whole_distribution['layerId'] == last_hidden_layerId].copy(deep=True)\n",
    "    # Filter the distribution on the last hidden layer\n",
    "    important_var_train_whole_last_hidden_distribution = train_whole_last_hidden_distribution[train_whole_last_hidden_distribution['nodeId'].isin(current_important_neuron_indices)]\n",
    "    # Map the node Id\n",
    "    train_important_var_distrib_by_class[classId] = important_var_train_whole_last_hidden_distribution.replace({\"nodeId\": important_var_map_dict_by_class[classId]}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the temporarily generated distribution files by class\n",
    "for classId in train_important_var_distrib_by_class:\n",
    "    save_df_to_csv(path_join(output_path, 'distribution_important_var_class_'+str(classId)+csv_ext), train_important_var_distrib_by_class[classId], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4fb8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the likelihood based on only the important variables\n",
    "train_whole_likelihood_important_vars = calculate_likelihood_with_important_vars(output_path, built_train_actLevels_by_class,\n",
    "                                             last_hidden_layerId)\n",
    "test_whole_likelihood_important_vars = calculate_likelihood_with_important_vars(output_path, built_test_actLevels_by_class,\n",
    "                                             last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb61d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the correctly predicted examples\n",
    "correct_train_whole_likelihood_important_var = train_whole_likelihood_important_vars[train_whole_likelihood_important_vars['classId'] == train_whole_likelihood_important_vars['predicted_classId']]\n",
    "# Calculate the distance (with only important variables) information\n",
    "class_dist_infos_important_var = build_layer_train_set_infos(correct_train_whole_likelihood_important_var, class_list)\n",
    "# Register the calculated information\n",
    "store_dict_as_json(path_join(output_path, 'train_dist_infos_important_var'+json_ext), class_dist_infos_important_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "train_whole_likelihood_important_vars = normalize_whole_distances(train_whole_likelihood_important_vars, class_dist_infos_important_var)\n",
    "test_whole_likelihood_important_vars = normalize_whole_distances(test_whole_likelihood_important_vars, class_dist_infos_important_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20539547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decisions to be filtered\n",
    "train_filtered_index_important_var = filter_decision_based_on_train_infos_norm_ver(train_whole_likelihood_important_vars, class_dist_infos_important_var, std_threshold_coeff)\n",
    "test_filtered_index_important_var = filter_decision_based_on_train_infos_norm_ver(test_whole_likelihood_important_vars, class_dist_infos_important_var, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf87953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original CIFAR-10 dataset filtering results (with all neurons)\n",
    "# Accuracy evaluation on the filtering\n",
    "built_cifar10_original_important_vars_filtering_result = []\n",
    "print('Evaluation with only the important neurons:')\n",
    "print('Training set:')\n",
    "built_cifar10_original_important_vars_filtering_result.append(['train',\n",
    "                                                *evaluate_filtering(train_whole_likelihood_important_vars, train_filtered_index_important_var, 'train')])\n",
    "print()\n",
    "print('Test set:')\n",
    "built_cifar10_original_important_vars_filtering_result.append(['test',\n",
    "                                                *evaluate_filtering(test_whole_likelihood_important_vars, test_filtered_index_important_var, 'test')])\n",
    "# Build the result dataframe\n",
    "cifar10_original_important_vars_OOD_df = pd.DataFrame(built_cifar10_original_important_vars_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_cifar10_important_vars_OOD_result.csv'), cifar10_original_important_vars_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original CIFAR-10 dataset filtering results (with all neurons)\n",
    "# Accuracy evaluation on the filtering\n",
    "built_cifar10_original_filtering_result = []\n",
    "print('Evaluation with all the neurons:')\n",
    "print('Training set:')\n",
    "built_cifar10_original_filtering_result.append(['train', *evaluate_filtering(all_train_whole_distances, all_train_filtered_index, 'train')])\n",
    "print()\n",
    "print('Test set:')\n",
    "built_cifar10_original_filtering_result.append(['test', *evaluate_filtering(test_whole_distances, test_filtered_index, 'test')])\n",
    "# Build the result dataframe\n",
    "cifar10_original_OOD_df = pd.DataFrame(built_cifar10_original_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_cifar10_OOD_result.csv'), cifar10_original_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test set whole likelihood distances\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_train_sobol_whole_likelihood.csv'), train_whole_likelihood_important_vars)\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_test_sobol_whole_likelihood.csv'), test_whole_likelihood_important_vars)\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_train_whole_likelihood.csv'), all_train_whole_distances)\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_test_whole_likelihood.csv'), test_whole_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287bc59",
   "metadata": {},
   "source": [
    "### *Cifar10 novelty experiments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the svhn dataset\n",
    "svhn_train_dataset, svhn_test_dataset = get_svhn_dataset_without_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader building\n",
    "# svhn_train_loader = create_loader_from_torch_dataset(svhn_train_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "svhn_test_loader = create_loader_from_torch_dataset(svhn_test_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dtd dataset\n",
    "dtd_train_dataset, dtd_test_dataset = get_dtd_dataset_resized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd56080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the train set to numpy array\n",
    "# no_divide_into_batch_dtd_train_loader = create_loader_from_torch_dataset(dtd_train_dataset, batch_size=len(dtd_train_dataset), shuffle=False, num_workers=0)\n",
    "# X_train_dtd = next(iter(no_divide_into_batch_dtd_train_loader))[0].numpy()\n",
    "# y_train_dtd = next(iter(no_divide_into_batch_dtd_train_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to numpy array\n",
    "no_divide_into_batch_dtd_test_loader = create_loader_from_torch_dataset(dtd_test_dataset, batch_size=len(dtd_test_dataset), shuffle=False, num_workers=0)\n",
    "X_test_dtd = next(iter(no_divide_into_batch_dtd_test_loader))[0].numpy()\n",
    "y_test_dtd = next(iter(no_divide_into_batch_dtd_test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e212011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dtd loaders (using random original labels (because they are not important))\n",
    "# dtd_train_loader = create_dataloader(X_train_dtd, np.random.randint(0, data_set_infos['nb_classes'], y_train_dtd.shape[0]), \n",
    "#                                      batch_size=torch_batch_size, shuffle=False, type_conversion=True)\n",
    "dtd_test_loader = create_dataloader(X_test_dtd, np.random.randint(0, data_set_infos['nb_classes'], y_test_dtd.shape[0]), \n",
    "                                     batch_size=torch_batch_size, shuffle=False, type_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the places365 dataset\n",
    "places_test_dataset = get_places_test_dataset_resized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c73f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to numpy array\n",
    "no_divide_into_batch_places_test_loader = create_loader_from_torch_dataset(places_test_dataset, batch_size=len(places_test_dataset), shuffle=False, num_workers=0)\n",
    "X_test_places = next(iter(no_divide_into_batch_places_test_loader))[0].numpy()\n",
    "y_test_places = next(iter(no_divide_into_batch_places_test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the places365 loaders (using random original labels (because they are not important))\n",
    "places_test_loader = create_dataloader(X_test_places, np.random.randint(0, data_set_infos['nb_classes'], y_test_places.shape[0]), \n",
    "                                     batch_size=torch_batch_size, shuffle=False, type_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb053a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary that contains all the OOD dataset loaders\n",
    "novelty_loaders = {}\n",
    "novelty_loaders['svhn'] = svhn_test_loader\n",
    "novelty_loaders['dtd'] = dtd_test_loader\n",
    "novelty_loaders['places'] = places_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e682a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the OMS datasets for generating the normalized feature vectors\n",
    "novelty_actLevels = {}\n",
    "for ood_type in novelty_loaders:\n",
    "    novelty_actLevels[ood_type] = obtain_activation_levels(trained_resnet,\n",
    "                                                           novelty_loaders[ood_type], ood_type + ' test',\n",
    "                                                           with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992415f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whole likelihood distances experiment\n",
    "# Calculate the likelihood on the last hidden layer\n",
    "novelty_whole_distances = {}\n",
    "for ood_type in novelty_actLevels:\n",
    "    novelty_whole_distances[ood_type] = layer_whole_likelihood_experim(novelty_actLevels[ood_type], last_hidden_layerId,\n",
    "                               train_distribution_filename, class_list, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for ood_type in novelty_whole_distances:\n",
    "    novelty_whole_distances[ood_type] = normalize_whole_distances(novelty_whole_distances[ood_type], class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66552bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for ood_type in novelty_whole_distances:\n",
    "    save_df_to_csv(path_join(output_path, trained_resnet_name+'_'+ood_type+'_whole_likelihood.csv'), novelty_whole_distances[ood_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd00c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "novelty_filtered_index = {}\n",
    "for ood_type in novelty_whole_distances:\n",
    "    novelty_filtered_index[ood_type] = filter_decision_based_on_train_infos_norm_ver(novelty_whole_distances[ood_type], class_dist_infos, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701ac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_novelty_filtering_result = []\n",
    "for ood_type in novelty_filtered_index:\n",
    "    print(ood_type, 'set:')\n",
    "    novelty_filtering_result = evaluate_filtering(novelty_whole_distances[ood_type], novelty_filtered_index[ood_type], ood_type)\n",
    "    print()\n",
    "    built_novelty_filtering_result.append([ood_type, *novelty_filtering_result])\n",
    "# Build the result dataframe\n",
    "novelty_OOD_df = pd.DataFrame(built_novelty_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_novelty_OOD_result.csv'), novelty_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70416d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the activation levels of the important variables for each class\n",
    "built_novelty_actLevels_by_class = {}\n",
    "for ood_type in novelty_actLevels:\n",
    "    built_novelty_actLevels_by_class[ood_type] = build_actLevel_important_vars(novelty_actLevels[ood_type], sorted_important_var_by_class, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the whole likelihood with the important variables\n",
    "novelty_whole_likelihood_important_vars = {}\n",
    "for ood_type in built_novelty_actLevels_by_class:\n",
    "    novelty_whole_likelihood_important_vars[ood_type] = calculate_likelihood_with_important_vars(output_path, built_novelty_actLevels_by_class[ood_type],\n",
    "                                                 last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83579ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for ood_type in novelty_whole_likelihood_important_vars:\n",
    "    novelty_whole_likelihood_important_vars[ood_type] = normalize_whole_distances(novelty_whole_likelihood_important_vars[ood_type], class_dist_infos_important_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115284da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for ood_type in novelty_whole_likelihood_important_vars:\n",
    "    save_df_to_csv(path_join(output_path, trained_resnet_name+'_'+ood_type+'_sobol_whole_likelihood.csv'), novelty_whole_likelihood_important_vars[ood_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b8a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "novelty_filtered_index_important_vars = {}\n",
    "for ood_type in novelty_whole_likelihood_important_vars:\n",
    "    novelty_filtered_index_important_vars[ood_type] = filter_decision_based_on_train_infos_norm_ver(novelty_whole_likelihood_important_vars[ood_type], class_dist_infos_important_var, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_novelty_important_vars_filtering_result = []\n",
    "for ood_type in novelty_filtered_index_important_vars:\n",
    "    print(ood_type, 'set:')\n",
    "    novelty_important_vars_filtering_result = evaluate_filtering(novelty_whole_likelihood_important_vars[ood_type], novelty_filtered_index_important_vars[ood_type], ood_type)\n",
    "    print()\n",
    "    built_novelty_important_vars_filtering_result.append([ood_type, *novelty_important_vars_filtering_result])\n",
    "# Build the result dataframe\n",
    "novelty_important_var_OOD_df = pd.DataFrame(built_novelty_important_vars_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_novelty_important_vars_OOD_result.csv'), novelty_important_var_OOD_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361efb3",
   "metadata": {},
   "source": [
    "### *Cifar10-c experiments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the cifar10-c dataset\n",
    "# Get the content in the folder\n",
    "cifar10_c_data_files = [file for file in contents_of_folder(cifar10_c_path) if np_ext in file]\n",
    "# The number of images at each level\n",
    "nb_image_by_level = 10000\n",
    "# Load all the files\n",
    "load_cifar10_c = {}\n",
    "for file in cifar10_c_data_files:\n",
    "    # We always use the transformation_type as variable name for the ease of coding (even if it could be just \"labels\")  \n",
    "    file_type = str_first_part_split_from_r(file)\n",
    "    current_data = np.load(path_join(cifar10_c_path, file))\n",
    "    nb_batchs = current_data.shape[0] / nb_image_by_level\n",
    "    for index in range(int(nb_batchs)):\n",
    "        current_batch_data = current_data[index*nb_image_by_level:(index+1)*nb_image_by_level]\n",
    "        load_cifar10_c[file_type+'_s'+str(index+1)] = current_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef17c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a CIFAR10-c example\n",
    "plt.imshow(load_cifar10_c['frost_s5'][3400], interpolation='nearest')\n",
    "plt.show()\n",
    "print(load_cifar10_c['labels_s5'][3400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ccf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataloader and evaluate the activation levels\n",
    "cifar10_c_actLevels = {}\n",
    "for transformation_type in load_cifar10_c:\n",
    "    if 'labels' not in transformation_type:\n",
    "        # Get the severe level\n",
    "        severity = str_second_part_split_from_r(transformation_type, delimiter='_')\n",
    "        # Get the current image array\n",
    "        transformed_image_array = load_cifar10_c[transformation_type]\n",
    "        # Reshape the numpy array to satisfy pytorch model requirements     \n",
    "        pytorch_transformed_image_array = transformed_image_array.transpose(0,3,1,2)\n",
    "        # Normalize the pixel values to (0,1) range\n",
    "        pytorch_transformed_image_array = pytorch_transformed_image_array / image_max_pix_val\n",
    "        # Build the loader         \n",
    "        current_transformed_loader = create_dataloader(pytorch_transformed_image_array, load_cifar10_c['labels_'+severity],\n",
    "                                                                   torch_batch_size, shuffle=False, type_conversion=True)\n",
    "        # Evaluate the activation levels\n",
    "        cifar10_c_actLevels[transformation_type] = obtain_activation_levels(trained_resnet,\n",
    "                                           current_transformed_loader, transformation_type, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50c6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Whole likelihood distances experiment\n",
    "# Calculate the likelihood on the last hidden layer\n",
    "cifar10_c_whole_distances = {}\n",
    "for transformation_type in cifar10_c_actLevels:\n",
    "    cifar10_c_whole_distances[transformation_type] = layer_whole_likelihood_experim(cifar10_c_actLevels[transformation_type], last_hidden_layerId,\n",
    "                               train_distribution_filename, class_list, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for transformation_type in cifar10_c_whole_distances:\n",
    "    cifar10_c_whole_distances[transformation_type] = normalize_whole_distances(cifar10_c_whole_distances[transformation_type], class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03206713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for transformation_type in cifar10_c_whole_distances:\n",
    "    save_df_to_csv(path_join(output_path, trained_resnet_name+'_'+transformation_type+'_whole_likelihood.csv'), cifar10_c_whole_distances[transformation_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "cifar10_c_filtered_index = {}\n",
    "for transformation_type in cifar10_c_whole_distances:\n",
    "    cifar10_c_filtered_index[transformation_type] = filter_decision_based_on_train_infos_norm_ver(cifar10_c_whole_distances[transformation_type], class_dist_infos, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8036419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_cifar10_c_filtering_result = []\n",
    "for transformation_type in cifar10_c_filtered_index:\n",
    "    print(transformation_type, 'set:')\n",
    "    cifar10_c_filtering_result = evaluate_filtering(cifar10_c_whole_distances[transformation_type], cifar10_c_filtered_index[transformation_type], transformation_type)\n",
    "    print()\n",
    "    built_cifar10_c_filtering_result.append([transformation_type, *cifar10_c_filtering_result])\n",
    "# Build the result dataframe\n",
    "cifar10_c_OOD_df = pd.DataFrame(built_cifar10_c_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_cifar10_c_OOD_result.csv'), cifar10_c_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the activation levels of the important variables for each class\n",
    "built_cifar10_c_actLevels_by_class = {}\n",
    "for transformation_type in cifar10_c_actLevels:\n",
    "    built_cifar10_c_actLevels_by_class[transformation_type] = build_actLevel_important_vars(cifar10_c_actLevels[transformation_type], sorted_important_var_by_class, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09611089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the whole likelihood with the important variables\n",
    "cifar10_c_whole_likelihood_important_vars = {}\n",
    "for transformation_type in built_cifar10_c_actLevels_by_class:\n",
    "    cifar10_c_whole_likelihood_important_vars[transformation_type] = calculate_likelihood_with_important_vars(output_path, built_cifar10_c_actLevels_by_class[transformation_type],\n",
    "                                                 last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for transformation_type in cifar10_c_whole_likelihood_important_vars:\n",
    "    cifar10_c_whole_likelihood_important_vars[transformation_type] = normalize_whole_distances(cifar10_c_whole_likelihood_important_vars[transformation_type], class_dist_infos_important_var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b824135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for transformation_type in cifar10_c_whole_likelihood_important_vars:\n",
    "    save_df_to_csv(path_join(output_path, trained_resnet_name+'_'+transformation_type+'_sobol_whole_likelihood.csv'), cifar10_c_whole_likelihood_important_vars[transformation_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05428dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "cifar10_c_filtered_index_important_vars = {}\n",
    "for transformation_type in cifar10_c_whole_likelihood_important_vars:\n",
    "    cifar10_c_filtered_index_important_vars[transformation_type] = filter_decision_based_on_train_infos_norm_ver(cifar10_c_whole_likelihood_important_vars[transformation_type], class_dist_infos_important_var, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_cifar10_c_important_vars_filtering_result = []\n",
    "for transformation_type in cifar10_c_filtered_index_important_vars:\n",
    "    print(transformation_type, 'set:')\n",
    "    cifar10_c_important_vars_filtering_result = evaluate_filtering(cifar10_c_whole_likelihood_important_vars[transformation_type], cifar10_c_filtered_index_important_vars[transformation_type], transformation_type)\n",
    "    print()\n",
    "    built_cifar10_c_important_vars_filtering_result.append([transformation_type, *cifar10_c_important_vars_filtering_result])\n",
    "# Build the result dataframe\n",
    "cifar10_c_important_var_OOD_df = pd.DataFrame(built_cifar10_c_important_vars_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_cifar10_c_important_vars_OOD_result.csv'), cifar10_c_important_var_OOD_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e083a4",
   "metadata": {},
   "source": [
    "### *Cifar10 adversarial attack experiments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The registrered original dataset for the attacks\n",
    "original_X = None\n",
    "original_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834da09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the adversarial attacks\n",
    "# Find the loaded trained resnet attack path\n",
    "trained_resnet_attack_path = None\n",
    "for attack_folder in contents_of_folder(adv_attack_path):\n",
    "    if trained_resnet_name in attack_folder:\n",
    "        trained_resnet_attack_path = path_join(adv_attack_path, attack_folder)\n",
    "        break\n",
    "# Load all the attacks\n",
    "loaded_attacks = {}\n",
    "for attack_set in contents_of_folder(trained_resnet_attack_path):\n",
    "    current_attack_type = str_first_part_split_from_l(attack_set)\n",
    "    current_attack_set_path = path_join(trained_resnet_attack_path, attack_set)\n",
    "    if current_attack_type == 'original':\n",
    "        X_file_path = path_join(current_attack_set_path, 'X.npy')\n",
    "        y_file_path = path_join(current_attack_set_path, 'y.npy')\n",
    "        original_X = np.load(X_file_path)\n",
    "        original_y = np.load(y_file_path)\n",
    "    else:\n",
    "        attack_file_path = path_join(current_attack_set_path, contents_of_folder(current_attack_set_path)[0])\n",
    "        loaded_attacks[current_attack_type] = np.load(attack_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2671816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the attack loaders\n",
    "attack_loaders = {}\n",
    "for attack_type in loaded_attacks:\n",
    "    attack_loaders[attack_type] = create_dataloader(loaded_attacks[attack_type], original_y, torch_batch_size, shuffle=False, type_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the attacks\n",
    "for attack_type in attack_loaders:\n",
    "    accuracy_eval(trained_resnet, attack_loaders[attack_type], set_name=attack_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the activation levels\n",
    "attack_actLevels = {}\n",
    "for attack_type in attack_loaders:\n",
    "    attack_actLevels[attack_type] = obtain_activation_levels(trained_resnet,\n",
    "                                       attack_loaders[attack_type], attack_type, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whole likelihood distances experiment\n",
    "# Calculate the likelihood on the last hidden layer\n",
    "attack_whole_distances = {}\n",
    "for attack_type in attack_actLevels:\n",
    "    attack_whole_distances[attack_type] = layer_whole_likelihood_experim(attack_actLevels[attack_type], last_hidden_layerId,\n",
    "                               train_distribution_filename, class_list, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5277816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for attack_type in attack_whole_distances:\n",
    "    attack_whole_distances[attack_type] = normalize_whole_distances(attack_whole_distances[attack_type], class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for attack_type in attack_whole_distances:\n",
    "    save_df_to_csv(path_join(output_path, trained_resnet_name+'_'+attack_type+'_whole_likelihood.csv'), attack_whole_distances[attack_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb236b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "attack_filtered_index = {}\n",
    "for attack_type in attack_whole_distances:\n",
    "    attack_filtered_index[attack_type] = filter_decision_based_on_train_infos_norm_ver(attack_whole_distances[attack_type], class_dist_infos, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_attack_filtering_result = []\n",
    "for attack_type in attack_filtered_index:\n",
    "    print(attack_type, 'set:')\n",
    "    attack_filtering_result = evaluate_filtering(attack_whole_distances[attack_type], attack_filtered_index[attack_type], attack_type)\n",
    "    print()\n",
    "    built_attack_filtering_result.append([attack_type, *attack_filtering_result])\n",
    "# Build the result dataframe\n",
    "attack_OOD_df = pd.DataFrame(built_attack_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_attack_OOD_result.csv'), attack_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79282e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the activation levels of the important variables for each class\n",
    "built_attack_actLevels_by_class = {}\n",
    "for attack_type in attack_actLevels:\n",
    "    built_attack_actLevels_by_class[attack_type] = build_actLevel_important_vars(attack_actLevels[attack_type], sorted_important_var_by_class, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the whole likelihood with the important variables\n",
    "attack_whole_likelihood_important_vars = {}\n",
    "for attack_type in built_attack_actLevels_by_class:\n",
    "    attack_whole_likelihood_important_vars[attack_type] = calculate_likelihood_with_important_vars(output_path, built_attack_actLevels_by_class[attack_type],\n",
    "                                                 last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for attack_type in attack_whole_likelihood_important_vars:\n",
    "    attack_whole_likelihood_important_vars[attack_type] = normalize_whole_distances(attack_whole_likelihood_important_vars[attack_type], class_dist_infos_important_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a32ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for attack_type in attack_whole_likelihood_important_vars:\n",
    "    save_df_to_csv(path_join(output_path, trained_resnet_name+'_'+attack_type+'_sobol_whole_likelihood.csv'), attack_whole_likelihood_important_vars[attack_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c35385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "attack_filtered_index_important_vars = {}\n",
    "for attack_type in attack_whole_likelihood_important_vars:\n",
    "    attack_filtered_index_important_vars[attack_type] = filter_decision_based_on_train_infos_norm_ver(attack_whole_likelihood_important_vars[attack_type], class_dist_infos_important_var, std_threshold_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_attack_important_vars_filtering_result = []\n",
    "for attack_type in attack_filtered_index_important_vars:\n",
    "    print(attack_type, 'set:')\n",
    "    attack_important_vars_filtering_result = evaluate_filtering(attack_whole_likelihood_important_vars[attack_type], attack_filtered_index_important_vars[attack_type], attack_type)\n",
    "    print()\n",
    "    built_attack_important_vars_filtering_result.append([attack_type, *attack_important_vars_filtering_result])\n",
    "# Build the result dataframe\n",
    "attack_important_var_OOD_df = pd.DataFrame(built_attack_important_vars_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_attack_important_vars_OOD_result.csv'), attack_important_var_OOD_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
