{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e88ded8",
   "metadata": {},
   "source": [
    "### *Module Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbfb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.display import display as ip_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbc82f",
   "metadata": {},
   "source": [
    "### *External Module Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_modules_path = '..\\\\nn_likelihood_modules'\n",
    "sys.path.append(external_modules_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_network_structure import *\n",
    "from common_imports import *\n",
    "from common_use_functions import *\n",
    "from constant import *\n",
    "from defined_data_structure import *\n",
    "from defined_network_structure import *\n",
    "from distribution_calculation import *\n",
    "from direct_inference_likelihood import *\n",
    "from experim_neural_network import *\n",
    "from experim_preparation import *\n",
    "from generate_activation_level import *\n",
    "from pytorch_model_predict import *\n",
    "from vector_preprocessing import *\n",
    "from ResNet import *\n",
    "from experim_ResNet import *\n",
    "from cifar_10_data_prep import *\n",
    "from pytorch_swintransformer_modified import *\n",
    "from tiny_imagenet_data_prep import *\n",
    "from sensitivity_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc22ef",
   "metadata": {},
   "source": [
    "### *GPU verification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nb_gpu = torch.cuda.device_count()\n",
    "if nb_gpu > 0:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701537f",
   "metadata": {},
   "source": [
    "### *Working directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current path\n",
    "current_path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c8167",
   "metadata": {},
   "source": [
    "### *Load configurations and data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All the parameters in this part should be configured\n",
    "\"\"\"\n",
    "# Experience path\n",
    "experim_path = current_path\n",
    "\n",
    "# File extensions\n",
    "json_ext = '.json'\n",
    "np_ext = '.npy'\n",
    "csv_ext = '.csv'\n",
    "\n",
    "# ResNet model prefix\n",
    "model_name_prefix = 'swin'\n",
    "\n",
    "# Image max pixel value\n",
    "image_max_pix_val = 255\n",
    "\n",
    "# Tested sets name\n",
    "train_set_name = 'train'\n",
    "test_set_name = 'test'\n",
    "valid_set_name = 'valid'\n",
    "input_extension = 'X'\n",
    "label_extension = 'Y'\n",
    "\n",
    "# Save paths\n",
    "model_save_path = path_join(experim_path, 'experim_models_swin')\n",
    "\n",
    "\"\"\"\n",
    "The following parameters should be configured according to your experiments\n",
    "\"\"\"\n",
    "\n",
    "# Trained model name \n",
    "trained_net_name = 'swin_b_v2_256' # You can select any model from the \"experim_models_swin\" folder \n",
    "\n",
    "# Network related params\n",
    "net_model_name = 'swin_b_v2_256'# The model name should be coherent with your chosen model\n",
    "\n",
    "# Tiny-imagenet datafolder path\n",
    "tiny_imagenet_path = 'D:\\\\Doctorat\\\\research\\\\Tiny_imagenet_experim\\\\tiny-imagenet-200-reform\\\\'\n",
    "tiny_imagenet_val_path = path_join(tiny_imagenet_path, 'reform_val')\n",
    "tiny_imagenet_train_path = path_join(tiny_imagenet_path, 'reform_train')\n",
    "\n",
    "# Tiny-imagenet-c datafolder path\n",
    "tiny_imagenet_c_path = 'D:\\\\Doctorat\\\\research\\\\Tiny_imagenet_experim\\\\Tiny-ImageNet-C-reform\\\\'\n",
    "\n",
    "# Dataset general informations\n",
    "data_set_infos = {\n",
    "    'nb_classes' : 200\n",
    "}\n",
    "\n",
    "# Distance decision filtering threshold\n",
    "std_threshold_coeff = 2\n",
    "\n",
    "# The method to determining the significant neurons (mean or most_common)\n",
    "sobol_filter_method = 'mean'\n",
    "\n",
    "# Rscript launch params\n",
    "Rscript_path = 'C:\\\\Program Files\\\\R\\\\R-4.3.3\\\\bin\\\\Rscript.exe'\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# The output folder\n",
    "output_path = path_join(experim_path, 'output')\n",
    "\n",
    "# Distribution file\n",
    "train_distribution_filename = path_join(output_path, 'distribution_' + train_set_name + '_set.csv')\n",
    "\n",
    "# Train distance information registration path\n",
    "train_dists_info_filename = 'train_dist_infos'\n",
    "train_dists_info_path = path_join(output_path, train_dists_info_filename + json_ext)\n",
    "\n",
    "# Build the class list\n",
    "class_list = list(range(data_set_infos['nb_classes']))\n",
    "\n",
    "# Batch size for the dataloader creation\n",
    "torch_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "create_directory(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ae1fb",
   "metadata": {},
   "source": [
    "### *Experiment preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55636cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tiny imagenet training set and test set\n",
    "tiny_imagenet_train_dataset, tiny_imagenet_test_dataset = get_tiny_imagenet_without_transform(tiny_imagenet_train_path, tiny_imagenet_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names of the filtering results\n",
    "column_names_OOD_filtering = ['transformation', 'nb_examples', 'nb_OOD', 'nb_InD', 'total_acc', 'OOD_acc', 'InD_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9931900",
   "metadata": {},
   "source": [
    "### *Load the trained Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network\n",
    "trained_net = load_model_by_net_name(model_save_path, trained_net_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849492d",
   "metadata": {},
   "source": [
    "### *Tiny imagenet dataset preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a90a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader building\n",
    "train_loader = create_loader_from_torch_dataset(tiny_imagenet_train_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = create_loader_from_torch_dataset(tiny_imagenet_test_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9019d",
   "metadata": {},
   "source": [
    "### *Distribution and Training set likelihood distance information generation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ea3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to gpu\n",
    "trained_net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86131e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activation levels\n",
    "train_actLevels = obtain_activation_levels(trained_net,\n",
    "                                           train_loader, train_set_name, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71993a-f15d-433b-b151-d04affcdc8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activation levels only on the correctly predicted examples\n",
    "correctly_predicted_bools = (train_actLevels['class'] == train_actLevels['predict_class']).reshape(-1)\n",
    "correctly_predicted_train_actLevels = copy.deepcopy(train_actLevels)\n",
    "for info in correctly_predicted_train_actLevels:\n",
    "    if info != 'actLevel':\n",
    "        correctly_predicted_train_actLevels[info] = correctly_predicted_train_actLevels[info][correctly_predicted_bools]\n",
    "    else:\n",
    "        for layerId in correctly_predicted_train_actLevels[info]:\n",
    "            correctly_predicted_train_actLevels[info][layerId] = correctly_predicted_train_actLevels[info][layerId][correctly_predicted_bools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c371a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution generation\n",
    "# Generate the distribution\n",
    "train_distributions = generate_distributions(correctly_predicted_train_actLevels, train_set_name, trained_net_name)\n",
    "# Save the distribution\n",
    "train_distribution_filename = save_distributions(output_path, train_distributions, train_set_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51800577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the likelihood information per class on the training set\n",
    "# Last hidden layer index and activation levels\n",
    "last_hidden_layerId = list(correctly_predicted_train_actLevels['actLevel'].keys())[-1]\n",
    "# Calculate the likelihood on the last hidden layer\n",
    "train_distrib_distances, train_distrib_map_dict = layer_ref_likelihood_experim(train_actLevels, last_hidden_layerId,\n",
    "                               train_distribution_filename, predicted_class=False, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the correctly predicted ones\n",
    "correct_train_last_hidden_distances = train_distrib_distances[train_distrib_distances['classId'] == train_distrib_distances['predicted_classId']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the information and save it\n",
    "# Information calculation\n",
    "class_dist_infos = build_layer_train_set_infos_ref_ver(correct_train_last_hidden_distances, class_list)\n",
    "# Register the calculated information\n",
    "store_dict_as_json(train_dists_info_path, class_dist_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b8bc8",
   "metadata": {},
   "source": [
    "### *Evaluate the likelihood on the whole training set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood on the last hidden layer\n",
    "train_distances, train_map_dict = layer_ref_likelihood_experim(train_actLevels, last_hidden_layerId,\n",
    "                               train_distribution_filename, predicted_class=True, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the whole distances\n",
    "train_distances = normalize_distances_ref_ver(train_distances, train_map_dict, class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83faf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decisions to be filtered\n",
    "train_filtered_index = filter_decision_based_on_train_infos_norm_and_ref_ver(train_distances, class_dist_infos,\n",
    "                                                                             std_threshold_coeff, original_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7198e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the filtered training set examples\n",
    "if len(train_filtered_index) != 0:\n",
    "    ip_display(train_distances.loc[train_filtered_index,:])\n",
    "else:\n",
    "    print('All the example images are identified as good inputs by the likelihood distance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb8c6c",
   "metadata": {},
   "source": [
    "### *Evaluate the likelihood on the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2001aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activation levels\n",
    "test_actLevels = obtain_activation_levels(trained_net,\n",
    "                                           test_loader, 'test', with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood on this layer\n",
    "test_distances, test_map_dict = layer_ref_likelihood_experim(test_actLevels, last_hidden_layerId,\n",
    "                               train_distribution_filename, predicted_class=True, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "test_distances = normalize_distances_ref_ver(test_distances, test_map_dict, class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906cf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decisions to be filtered\n",
    "test_filtered_index = filter_decision_based_on_train_infos_norm_and_ref_ver(test_distances, class_dist_infos,\n",
    "                                                                             std_threshold_coeff, original_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the filtered test examples\n",
    "if len(test_filtered_index) != 0:\n",
    "    ip_display(test_distances.loc[test_filtered_index,:])\n",
    "else:\n",
    "    print('All the example images are identified as good inputs by the likelihood distance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd61ec",
   "metadata": {},
   "source": [
    "### *Sobol index evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding data\n",
    "last_hidden_actLevels = train_actLevels['actLevel'][last_hidden_layerId][correctly_predicted_bools.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last layer parameters\n",
    "model_params = get_model_parameters(trained_net, to_numpy=True)\n",
    "final_linear_params = model_params['head_out.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b055e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of weight and input\n",
    "# Copy the original activation levels and weights\n",
    "normalized_last_hidden_actLevel = copy.deepcopy(last_hidden_actLevels)\n",
    "upped_final_linear_params = copy.deepcopy(final_linear_params)\n",
    "# Check the min and max values of each neuron\n",
    "last_hidden_actLevel_max = np.max(last_hidden_actLevels, axis=0)\n",
    "last_hidden_actLevel_min = np.min(last_hidden_actLevels, axis=0) # Not used, just for verification\n",
    "# Iterate over the maximum values and normalize the input\n",
    "for index, neuron_max in enumerate(last_hidden_actLevel_max):\n",
    "    if neuron_max != 0:\n",
    "        normalized_last_hidden_actLevel[:,index] = normalized_last_hidden_actLevel[:,index] / neuron_max\n",
    "        upped_final_linear_params['weight'][:,index] = upped_final_linear_params['weight'][:,index] * neuron_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f70433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final linear parameters per class and assign the real data\n",
    "final_linear_param_per_class = build_per_class_linear_model(upped_final_linear_params)\n",
    "data = normalized_last_hidden_actLevel\n",
    "# Number of variables\n",
    "nb_vars = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1800dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the X and y for the sobol index evaluation in R\n",
    "# Get the neuron names\n",
    "neuron_names = ['neuron_'+str(index) for index in range(last_hidden_actLevels.shape[1])]\n",
    "# Build the X dataframe\n",
    "R_X = pd.DataFrame(data, columns=neuron_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd985f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataframe that stores weights and bias\n",
    "R_network_params_data = []\n",
    "R_network_params_columns = ['classId', *(['weight_'+str(index) for index in range(len(final_linear_param_per_class[0]['weight']))]), 'bias']\n",
    "for classId in final_linear_param_per_class:\n",
    "    R_network_params_data.append([classId, *list(final_linear_param_per_class[classId]['weight']), final_linear_param_per_class[classId]['bias']])\n",
    "R_network_params = pd.DataFrame(R_network_params_data, columns=R_network_params_columns)\n",
    "save_df_to_csv(path_join(output_path, 'R_network_params.csv'),R_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b47a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random samples from R_X\n",
    "A_index = generate_sample_index(data, N, replace=False)\n",
    "B_index = generate_sample_index_exclude_items(data, N, A_index, replace=False)\n",
    "R_X_A = R_X.loc[A_index,:].copy(deep=True).reset_index(drop=True)\n",
    "R_X_B = R_X.loc[B_index,:].copy(deep=True).reset_index(drop=True)\n",
    "# Save the random samples from R_X\n",
    "save_df_to_csv(path_join(output_path, 'R_X_A.csv'),R_X_A)\n",
    "save_df_to_csv(path_join(output_path, 'R_X_B.csv'),R_X_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could choose sobolrank (only first order indices), sobolEff (first and total indices) or shapleysobol_knn (first and total indices)\n",
    "R_sobol_method = 'sobolEff'\n",
    "R_sobol_script = path_join(experim_path, R_sobol_method+'_eval.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the R script (The path to read the data is given as arguments (i.e., experim_path+'\\\\'))\n",
    "ouput_R = run([Rscript_path, '--vanilla', R_sobol_script, output_path+'\\\\', str(data_set_infos['nb_classes'])], shell=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the important variables per class\n",
    "# Initialize the determined variables as None\n",
    "important_variables_per_class = None\n",
    "if R_sobol_method == 'sobolEff':\n",
    "    # Load the sobol indices (first and total) and convert it to a dictionary\n",
    "    R_first_order_sobol_per_class_dict = {}\n",
    "    R_total_order_sobol_per_class_dict = {}\n",
    "    for classId in class_list:\n",
    "        R_current_class_first_order_sobol_indices = read_csv_to_pd_df(path_join(output_path, R_sobol_method+'_fs_'+str(classId)+csv_ext))\n",
    "        R_current_class_total_order_sobol_indices = read_csv_to_pd_df(path_join(output_path, R_sobol_method+'_tt_'+str(classId)+csv_ext))\n",
    "        R_first_order_sobol_per_class_dict[classId] = R_current_class_first_order_sobol_indices['S.original'].to_dict()\n",
    "        R_total_order_sobol_per_class_dict[classId] = R_current_class_total_order_sobol_indices['S.original'].to_dict()\n",
    "    # Get the important variables per class\n",
    "    important_variables_per_class = important_variables_R_first_and_total_order_analysis(R_first_order_sobol_per_class_dict, \n",
    "                                                                                         R_total_order_sobol_per_class_dict,\n",
    "                                                                                         class_list, filter_method=sobol_filter_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d060fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataframe that contains the number of neurons\n",
    "determined_nb_important_vars_per_class = []\n",
    "for classId in important_variables_per_class:\n",
    "    class_nb_important_vars = len(list(important_variables_per_class[classId].keys()))\n",
    "    determined_nb_important_vars_per_class.append([classId, nb_vars, class_nb_important_vars])\n",
    "determined_nb_important_vars_per_class_df = pd.DataFrame(determined_nb_important_vars_per_class, columns=['classId', 'nb_neurons', 'nb_important_neurons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e621ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the important variables\n",
    "store_dict_as_json(path_join(output_path, trained_net_name+'_important_neurons.json'), important_variables_per_class)\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_nb_important_neurons.csv'), determined_nb_important_vars_per_class_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57224d4f",
   "metadata": {},
   "source": [
    "### *Likelihood calculation based only on the important variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847d199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sorted total important variable(neuron) indices\n",
    "sorted_important_var_by_class = {}\n",
    "for classId in important_variables_per_class:\n",
    "    sorted_important_var_by_class[classId] = sorted(list(important_variables_per_class[classId]))\n",
    "# Build the mapping dictionary to modify neuron indices\n",
    "important_var_map_dict_by_class = {}\n",
    "for classId in sorted_important_var_by_class:\n",
    "    important_var_map_dict_by_class[classId] = build_map_to_index_dict(sorted_important_var_by_class[classId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb71999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the training set distribution only for the important neurons\n",
    "# Read the training set distribution\n",
    "train_whole_distribution = pd.read_csv(train_distribution_filename, sep=' ')\n",
    "# Take the distribution for different classes\n",
    "train_important_var_distrib_by_class = {}\n",
    "for classId in sorted_important_var_by_class:\n",
    "    # Get the important variables of the current class\n",
    "    current_important_neuron_indices = sorted_important_var_by_class[classId]\n",
    "    # Take only the last hidden layer distribution (We calculate the likelihood only based on this)\n",
    "    train_whole_last_hidden_distribution = train_whole_distribution[train_whole_distribution['layerId'] == last_hidden_layerId].copy(deep=True)\n",
    "    # Filter the distribution on the last hidden layer\n",
    "    important_var_train_whole_last_hidden_distribution = train_whole_last_hidden_distribution[train_whole_last_hidden_distribution['nodeId'].isin(current_important_neuron_indices)]\n",
    "    # Map the node Id\n",
    "    train_important_var_distrib_by_class[classId] = important_var_train_whole_last_hidden_distribution.replace({\"nodeId\": important_var_map_dict_by_class[classId]}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the temporarily generated distribution files by class\n",
    "for classId in train_important_var_distrib_by_class:\n",
    "    save_df_to_csv(path_join(output_path, 'distribution_important_var_class_'+str(classId)+csv_ext), train_important_var_distrib_by_class[classId], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eb085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood based on only the important variables according to the original class\n",
    "train_distrib_likelihood_important_vars, train_distrib_map_dict_important_vars = calculate_likelihood_with_important_vars_saving_memory_ref_ver(output_path, train_actLevels,\n",
    "                                                                                               sorted_important_var_by_class,\n",
    "                                                                                               last_hidden_layerId, predicted_class=False,\n",
    "                                                                                               use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5fc601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the correctly predicted examples\n",
    "correct_train_likelihood_important_vars = train_distrib_likelihood_important_vars[train_distrib_likelihood_important_vars['classId'] == train_distrib_likelihood_important_vars['predicted_classId']]\n",
    "# Calculate the distance (with only important variables) information\n",
    "class_dist_infos_important_vars = build_layer_train_set_infos_ref_ver(correct_train_likelihood_important_vars, class_list)\n",
    "# Register the calculated information\n",
    "store_dict_as_json(path_join(output_path, 'train_dist_infos_important_var'+json_ext), class_dist_infos_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the likelihood based on only the important variables\n",
    "train_likelihood_important_vars, train_map_dict_important_vars = calculate_likelihood_with_important_vars_saving_memory_ref_ver(output_path, train_actLevels,\n",
    "                                                                                               sorted_important_var_by_class,\n",
    "                                                                                               last_hidden_layerId, predicted_class=True,\n",
    "                                                                                               use_absolute_module_path=True)\n",
    "test_likelihood_important_vars, test_map_dict_important_vars = calculate_likelihood_with_important_vars_saving_memory_ref_ver(output_path, test_actLevels,\n",
    "                                                                                              sorted_important_var_by_class,\n",
    "                                                                                              last_hidden_layerId, predicted_class=True,\n",
    "                                                                                              use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "train_likelihood_important_vars = normalize_distances_ref_ver(train_likelihood_important_vars, \n",
    "                                                              train_map_dict_important_vars, class_dist_infos_important_vars)\n",
    "test_likelihood_important_vars = normalize_distances_ref_ver(test_likelihood_important_vars,\n",
    "                                                             test_map_dict_important_vars, class_dist_infos_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20539547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decisions to be filtered\n",
    "train_filtered_index_important_vars = filter_decision_based_on_train_infos_norm_and_ref_ver(train_likelihood_important_vars,\n",
    "                                                                                           class_dist_infos_important_vars,\n",
    "                                                                                           std_threshold_coeff, original_class=False)\n",
    "test_filtered_index_important_vars = filter_decision_based_on_train_infos_norm_and_ref_ver(test_likelihood_important_vars,\n",
    "                                                                                          class_dist_infos_important_vars,\n",
    "                                                                                          std_threshold_coeff, original_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf87953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Tiny-imagenet dataset filtering results (with only the signifiant neurons)\n",
    "# Accuracy evaluation on the filtering\n",
    "built_tiny_imagenet_original_important_vars_filtering_result = []\n",
    "print('Evaluation with only the important neurons:')\n",
    "print('Training set:')\n",
    "built_tiny_imagenet_original_important_vars_filtering_result.append(['train',\n",
    "                                                *evaluate_filtering(train_likelihood_important_vars, train_filtered_index_important_vars, 'train')])\n",
    "print()\n",
    "print('Test set:')\n",
    "built_tiny_imagenet_original_important_vars_filtering_result.append(['test',\n",
    "                                                *evaluate_filtering(test_likelihood_important_vars, test_filtered_index_important_vars, 'test')])\n",
    "# Build the result dataframe\n",
    "tiny_imagenet_original_important_vars_OOD_df = pd.DataFrame(built_tiny_imagenet_original_important_vars_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_tiny_imagenet_important_vars_OOD_result.csv'), tiny_imagenet_original_important_vars_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Tiny-imagenet dataset filtering results (with all neurons)\n",
    "# Accuracy evaluation on the filtering\n",
    "built_tiny_imagenet_original_filtering_result = []\n",
    "print('Evaluation with all the neurons:')\n",
    "print('Training set:')\n",
    "built_tiny_imagenet_original_filtering_result.append(['train', *evaluate_filtering(train_distances, train_filtered_index, 'train')])\n",
    "print()\n",
    "print('Test set:')\n",
    "built_tiny_imagenet_original_filtering_result.append(['test', *evaluate_filtering(test_distances, test_filtered_index, 'test')])\n",
    "# Build the result dataframe\n",
    "tiny_imagenet_original_OOD_df = pd.DataFrame(built_tiny_imagenet_original_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_tiny_imagenet_OOD_result.csv'), tiny_imagenet_original_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test set whole likelihood distances\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_train_sobol_likelihood.csv'), train_likelihood_important_vars)\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_test_sobol_likelihood.csv'), test_likelihood_important_vars)\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_train_likelihood.csv'), train_distances)\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_test_likelihood.csv'), test_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361efb3",
   "metadata": {},
   "source": [
    "### *Tiny-imagenet-c experiments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the Tiny-imagnet-c dataset\n",
    "# Get the content in the folder\n",
    "transformation_types = [transformation_type for transformation_type in contents_of_folder(tiny_imagenet_c_path)]\n",
    "# Load all the transformation datasets\n",
    "load_tiny_imagenet_c_datasets = {}\n",
    "for transformation_type in transformation_types:\n",
    "    load_tiny_imagenet_c_datasets[transformation_type] = datasets.ImageFolder(\n",
    "                                                        root=path_join(tiny_imagenet_c_path, transformation_type),\n",
    "                                                        transform=v2.Compose([v2.ToTensor()]),\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef17c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a Tiny-imagenet-c example\n",
    "plt.imshow(load_tiny_imagenet_c_datasets['brightness_2'][1300][0].permute(1,2,0), interpolation='nearest')\n",
    "plt.show()\n",
    "print(load_tiny_imagenet_c_datasets['brightness_2'][1300][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ccf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataloader and evaluate the activation levels\n",
    "tiny_imagenet_c_actLevels = {}\n",
    "for transformation_type in load_tiny_imagenet_c_datasets:\n",
    "    # Build the loader         \n",
    "    current_transformed_loader = create_loader_from_torch_dataset(load_tiny_imagenet_c_datasets[transformation_type],\n",
    "                                                                  batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "    # Evaluate the activation levels\n",
    "    tiny_imagenet_c_actLevels[transformation_type] = obtain_activation_levels(trained_net,\n",
    "                                       current_transformed_loader, transformation_type, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50c6bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Whole likelihood distances experiment\n",
    "# Calculate the likelihood on the last hidden layer\n",
    "tiny_imagenet_c_distances = {}\n",
    "tiny_imagenet_c_map_dicts = {}\n",
    "for transformation_type in tiny_imagenet_c_actLevels:\n",
    "    tiny_imagenet_c_distances[transformation_type], tiny_imagenet_c_map_dicts[transformation_type] = layer_ref_likelihood_experim(tiny_imagenet_c_actLevels[transformation_type], last_hidden_layerId,\n",
    "                               train_distribution_filename, predicted_class=True, use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for transformation_type in tiny_imagenet_c_distances:\n",
    "    tiny_imagenet_c_distances[transformation_type] = normalize_distances_ref_ver(tiny_imagenet_c_distances[transformation_type],\n",
    "                                                                                 tiny_imagenet_c_map_dicts[transformation_type],\n",
    "                                                                                 class_dist_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03206713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for transformation_type in tiny_imagenet_c_distances:\n",
    "    save_df_to_csv(path_join(output_path, trained_net_name+'_'+transformation_type+'_likelihood.csv'), tiny_imagenet_c_distances[transformation_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "tiny_imagenet_c_filtered_index = {}\n",
    "for transformation_type in tiny_imagenet_c_distances:\n",
    "    tiny_imagenet_c_filtered_index[transformation_type] = filter_decision_based_on_train_infos_norm_and_ref_ver(tiny_imagenet_c_distances[transformation_type],\n",
    "                                                                                                                class_dist_infos,\n",
    "                                                                                                                std_threshold_coeff,\n",
    "                                                                                                                original_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8036419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_tiny_imagenet_c_filtering_result = []\n",
    "for transformation_type in tiny_imagenet_c_filtered_index:\n",
    "    print(transformation_type, 'set:')\n",
    "    tiny_imagenet_c_filtering_result = evaluate_filtering(tiny_imagenet_c_distances[transformation_type], tiny_imagenet_c_filtered_index[transformation_type], transformation_type)\n",
    "    print()\n",
    "    built_tiny_imagenet_c_filtering_result.append([transformation_type, *tiny_imagenet_c_filtering_result])\n",
    "# Build the result dataframe\n",
    "tiny_imagenet_c_OOD_df = pd.DataFrame(built_tiny_imagenet_c_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_tiny_imagenet_c_OOD_result.csv'), tiny_imagenet_c_OOD_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09611089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the whole likelihood with the important variables\n",
    "tiny_imagenet_c_likelihood_important_vars = {}\n",
    "tiny_imagenet_c_map_dict_important_vars = {}\n",
    "for transformation_type in tiny_imagenet_c_actLevels:\n",
    "    tiny_imagenet_c_likelihood_important_vars[transformation_type], tiny_imagenet_c_map_dict_important_vars[transformation_type] = calculate_likelihood_with_important_vars_saving_memory_ref_ver(output_path, \n",
    "                                                                                               tiny_imagenet_c_actLevels[transformation_type],\n",
    "                                                                                               sorted_important_var_by_class,\n",
    "                                                                                               last_hidden_layerId, predicted_class=True,\n",
    "                                                                                               use_absolute_module_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distances\n",
    "for transformation_type in tiny_imagenet_c_likelihood_important_vars:\n",
    "    tiny_imagenet_c_likelihood_important_vars[transformation_type] = normalize_distances_ref_ver(tiny_imagenet_c_likelihood_important_vars[transformation_type], \n",
    "                                                              tiny_imagenet_c_map_dict_important_vars[transformation_type], class_dist_infos_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b824135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the whole likelihood dataframes\n",
    "for transformation_type in tiny_imagenet_c_likelihood_important_vars:\n",
    "    save_df_to_csv(path_join(output_path, trained_net_name+'_'+transformation_type+'_sobol_likelihood.csv'), tiny_imagenet_c_likelihood_important_vars[transformation_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05428dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood filtering\n",
    "tiny_imagenet_c_filtered_index_important_vars = {}\n",
    "for transformation_type in tiny_imagenet_c_likelihood_important_vars:\n",
    "    tiny_imagenet_c_filtered_index_important_vars[transformation_type] = filter_decision_based_on_train_infos_norm_and_ref_ver(tiny_imagenet_c_likelihood_important_vars[transformation_type],\n",
    "                                                                                           class_dist_infos_important_vars,\n",
    "                                                                                           std_threshold_coeff, original_class=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy evaluation on the filtering\n",
    "built_tiny_imagenet_c_important_vars_filtering_result = []\n",
    "for transformation_type in tiny_imagenet_c_filtered_index_important_vars:\n",
    "    print(transformation_type, 'set:')\n",
    "    tiny_imagenet_c_important_vars_filtering_result = evaluate_filtering(tiny_imagenet_c_likelihood_important_vars[transformation_type], tiny_imagenet_c_filtered_index_important_vars[transformation_type], transformation_type)\n",
    "    print()\n",
    "    built_tiny_imagenet_c_important_vars_filtering_result.append([transformation_type, *tiny_imagenet_c_important_vars_filtering_result])\n",
    "# Build the result dataframe\n",
    "tiny_imagenet_c_important_var_OOD_df = pd.DataFrame(built_tiny_imagenet_c_important_vars_filtering_result, columns=column_names_OOD_filtering)\n",
    "# Save the result dataframe\n",
    "save_df_to_csv(path_join(output_path, trained_net_name+'_tiny_imagenet_c_important_vars_OOD_result.csv'), tiny_imagenet_c_important_var_OOD_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
