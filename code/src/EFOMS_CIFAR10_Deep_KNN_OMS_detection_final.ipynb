{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e88ded8",
   "metadata": {},
   "source": [
    "### *Module Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbfb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import faiss\n",
    "from subprocess import PIPE, run\n",
    "from IPython.display import display as ip_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbc82f",
   "metadata": {},
   "source": [
    "### *External Module Loading*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_modules_path = '..\\\\nn_likelihood_modules'\n",
    "sys.path.append(external_modules_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_network_structure import *\n",
    "from common_imports import *\n",
    "from common_use_functions import *\n",
    "from constant import *\n",
    "from defined_data_structure import *\n",
    "from defined_network_structure import *\n",
    "from experim_neural_network import *\n",
    "from experim_preparation import *\n",
    "from generate_activation_level import *\n",
    "from pytorch_model_predict import *\n",
    "from vector_preprocessing import *\n",
    "from ResNet import *\n",
    "from experim_ResNet import *\n",
    "from cifar_10_data_prep import *\n",
    "from sensitivity_analysis import *\n",
    "from deep_KNN import *\n",
    "from novelty_data_prep import *\n",
    "from activation_level_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc22ef",
   "metadata": {},
   "source": [
    "### *GPU verification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nb_gpu = torch.cuda.device_count()\n",
    "if nb_gpu > 0:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0701537f",
   "metadata": {},
   "source": [
    "### *Working directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current path\n",
    "current_path = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c8167",
   "metadata": {},
   "source": [
    "### *Load configurations and data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All the parameters in this part should be configured\n",
    "\"\"\"\n",
    "# Experience path\n",
    "experim_path = current_path\n",
    "\n",
    "# File extensions\n",
    "json_ext = '.json'\n",
    "np_ext = '.npy'\n",
    "csv_ext = '.csv'\n",
    "\n",
    "# ResNet model prefix\n",
    "model_name_prefix = 'cifar10'\n",
    "\n",
    "# Image max pixel value\n",
    "image_max_pix_val = 255\n",
    "\n",
    "# Tested sets name\n",
    "train_set_name = 'train'\n",
    "test_set_name = 'test'\n",
    "valid_set_name = 'valid'\n",
    "input_extension = 'X'\n",
    "label_extension = 'Y'\n",
    "\n",
    "# Save paths\n",
    "model_save_path = path_join(experim_path, 'experim_models_resnet')\n",
    "\n",
    "# Adversarial attack path\n",
    "adv_attack_path = path_join(experim_path, 'experim_resnet_attack')\n",
    "\n",
    "\"\"\"\n",
    "The following parameters should be configured according to your experiments\n",
    "\"\"\"\n",
    "\n",
    "# Trained model name \n",
    "trained_resnet_name = 'cifar10_resnet50_938' # You can select any model from the \"experim_models_resnet\" folder\n",
    "\n",
    "# Indices filenames\n",
    "train_indices_filename = 'train_indices_938' # The train indices should be coherent with your chosen model\n",
    "valid_indices_filename = 'valid_indices_938' # The valid indices should be coherent with your chosen model\n",
    "\n",
    "# ResNet related params\n",
    "resnet_model_name = 'resnet50'# The model name should be coherent with your chosen model\n",
    "\n",
    "# Cifar10-c datafolder path\n",
    "cifar10_c_path = 'D:\\\\Doctorat\\\\research\\\\oms_detection_experim\\\\CIFAR-10-C\\CIFAR-10-C\\\\'\n",
    "\n",
    "# Dataset general informations\n",
    "data_set_infos = {\n",
    "    'nb_classes' : 10\n",
    "}\n",
    "\n",
    "# Distance decision filtering threshold\n",
    "std_threshold_coeff = 2\n",
    "\n",
    "# The method to determining the significant neurons (mean or most_common)\n",
    "sobol_filter_method = 'most_common'\n",
    "\n",
    "# The divide factor used for \"top portion\" and \"beside end portion\" methods which determine the significant neurons\n",
    "sobol_divide_factor = 4\n",
    "\n",
    "# Rscript launch params\n",
    "Rscript_path = 'C:\\\\Program Files\\\\R\\\\R-4.3.3\\\\bin\\\\Rscript.exe'\n",
    "\n",
    "# The number of considered k-nearst neighbors\n",
    "k = 50\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# The output folder\n",
    "output_path = path_join(experim_path, 'output')\n",
    "\n",
    "# Build the class list\n",
    "class_list = list(range(data_set_infos['nb_classes']))\n",
    "\n",
    "# Batch size for the dataloader creation\n",
    "torch_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folders\n",
    "create_directory(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1ae1fb",
   "metadata": {},
   "source": [
    "### *Experiment preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55636cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "cifar10_train_dataset, cifar10_test_dataset = get_cifar10_dataset_without_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names of the filtering results\n",
    "column_names_OOD_filtering = ['transformation', 'nb_examples', 'nb_OOD', 'nb_InD', 'total_acc', 'OOD_acc', 'InD_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9931900",
   "metadata": {},
   "source": [
    "### *Load the trained ResNet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the resnet\n",
    "trained_resnet = load_model_by_net_name(model_save_path, trained_resnet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a79b0",
   "metadata": {},
   "source": [
    "### *Move the model to GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to gpu\n",
    "trained_resnet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849492d",
   "metadata": {},
   "source": [
    "### *Cifar10 dataset preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd46a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train valid indices\n",
    "train_indices = load_json(open(path_join(model_save_path, train_indices_filename+json_ext)))\n",
    "valid_indices = load_json(open(path_join(model_save_path, valid_indices_filename+json_ext)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the subsets\n",
    "cifar10_real_train_dataset = Subset(cifar10_train_dataset, train_indices)\n",
    "cifar10_valid_dataset = Subset(cifar10_train_dataset, valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a90a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader building\n",
    "train_loader = create_loader_from_torch_dataset(cifar10_real_train_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "valid_loader = create_loader_from_torch_dataset(cifar10_valid_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = create_loader_from_torch_dataset(cifar10_test_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed01114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training set to numpy array\n",
    "no_divide_into_batch_train_loader = create_loader_from_torch_dataset(cifar10_real_train_dataset, batch_size=len(cifar10_real_train_dataset), shuffle=False, num_workers=0)\n",
    "cifar10_train_X = next(iter(no_divide_into_batch_train_loader))[0].numpy()\n",
    "cifar10_train_y = next(iter(no_divide_into_batch_train_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to numpy array\n",
    "no_divide_into_batch_test_loader = create_loader_from_torch_dataset(cifar10_test_dataset, batch_size=len(cifar10_test_dataset), shuffle=False, num_workers=0)\n",
    "X_test = next(iter(no_divide_into_batch_test_loader))[0].numpy()\n",
    "y_test = next(iter(no_divide_into_batch_test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13de364",
   "metadata": {},
   "source": [
    "### *Evaluate the activation levels for the original training and test sets of CIFAR-10*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training set activation levels \n",
    "train_actLevels = obtain_activation_levels(trained_resnet,\n",
    "                                           train_loader, 'train', with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test set activation levels \n",
    "test_actLevels = obtain_activation_levels(trained_resnet,\n",
    "                                           test_loader, 'test', with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817aa90e",
   "metadata": {},
   "source": [
    "### *Sobol index evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last hidden layer Id\n",
    "last_hidden_layerId = list(train_actLevels['actLevel'].keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60628992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correctly predicted index\n",
    "correctly_predicted_bools = train_actLevels['class'] == train_actLevels['predict_class']\n",
    "# Get the corresponding data\n",
    "last_hidden_actLevels = train_actLevels['actLevel'][last_hidden_layerId][correctly_predicted_bools.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last layer parameters\n",
    "model_params = get_model_parameters(trained_resnet, to_numpy=True)\n",
    "final_linear_params = model_params['linear_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08591daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of weight and input\n",
    "# Copy the original activation levels and weights\n",
    "normalized_last_hidden_actLevel = copy.deepcopy(last_hidden_actLevels)\n",
    "upped_final_linear_params = copy.deepcopy(final_linear_params)\n",
    "# Check the min and max values of each neuron\n",
    "last_hidden_actLevel_max = np.max(last_hidden_actLevels, axis=0)\n",
    "last_hidden_actLevel_min = np.min(last_hidden_actLevels, axis=0) # Not used, just for verification\n",
    "# Iterate over the maximum values and normalize the input\n",
    "for index, neuron_max in enumerate(last_hidden_actLevel_max):\n",
    "    if neuron_max != 0:\n",
    "        normalized_last_hidden_actLevel[:,index] = normalized_last_hidden_actLevel[:,index] / neuron_max\n",
    "        upped_final_linear_params['weight'][:,index] = upped_final_linear_params['weight'][:,index] * neuron_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a670e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the final linear parameters per class and assign the real data\n",
    "used_linear_params = upped_final_linear_params\n",
    "data = normalized_last_hidden_actLevel\n",
    "# Number of variables\n",
    "nb_vars = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1646e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the X and y for the sobol index evaluation in R\n",
    "# Get the neuron names\n",
    "neuron_names = ['neuron_'+str(index) for index in range(last_hidden_actLevels.shape[1])]\n",
    "# Build the X dataframe\n",
    "R_X = pd.DataFrame(data, columns=neuron_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataframe that stores weights and bias\n",
    "R_network_params_weight_columns = [*(['weight_'+str(index) for index in range(used_linear_params['weight'].shape[1])])]\n",
    "R_network_params_bias_columns = [*(['bias_'+str(index) for index in range(used_linear_params['bias'].shape[0])])]\n",
    "R_network_params_weights = pd.DataFrame(used_linear_params['weight'], columns=R_network_params_weight_columns)\n",
    "R_network_params_bias = pd.DataFrame(used_linear_params['bias'].reshape(1,-1), columns=R_network_params_bias_columns)\n",
    "save_df_to_csv(path_join(output_path, 'R_network_params_weight.csv'),R_network_params_weights)\n",
    "save_df_to_csv(path_join(output_path, 'R_network_params_bias.csv'),R_network_params_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281fc4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb24ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random samples from R_X\n",
    "A_index = generate_sample_index(data, N, replace=False)\n",
    "B_index = generate_sample_index_exclude_items(data, N, A_index, replace=False)\n",
    "R_X_A = R_X.loc[A_index,:].copy(deep=True).reset_index(drop=True)\n",
    "R_X_B = R_X.loc[B_index,:].copy(deep=True).reset_index(drop=True)\n",
    "# Save the random samples from R_X\n",
    "save_df_to_csv(path_join(output_path, 'R_X_A.csv'),R_X_A)\n",
    "save_df_to_csv(path_join(output_path, 'R_X_B.csv'),R_X_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c28b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could choose sobolrank (only first order indices), sobolEff (first and total indices) or shapleysobol_knn (first and total indices)\n",
    "R_sobol_method = 'sobolmultout'\n",
    "R_sobol_script = path_join(experim_path, R_sobol_method+'_eval.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the R script (The path to read the data is given as arguments (i.e., experim_path+'\\\\'))\n",
    "ouput_R = run([Rscript_path, '--vanilla', R_sobol_script, output_path+'\\\\', str(data_set_infos['nb_classes'])], shell=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the important variables per class\n",
    "# Initialize the determined variables as None\n",
    "important_variables = None\n",
    "if R_sobol_method == 'sobolmultout':\n",
    "    # Load the sobol indices (first and total) and convert it to a dictionary\n",
    "    R_first_order_sobol_indices = read_csv_to_pd_df(path_join(output_path, R_sobol_method+'_fs'+csv_ext))\n",
    "    R_total_order_sobol_indices = read_csv_to_pd_df(path_join(output_path, R_sobol_method+'_tt'+csv_ext))\n",
    "    R_first_order_sobol_dict = R_first_order_sobol_indices['original'].to_dict()\n",
    "    R_total_order_sobol_dict = R_total_order_sobol_indices['original'].to_dict()\n",
    "    # Get the important variables per class\n",
    "    important_variables = important_variables_R_first_and_total_order_analysis_multout_ver(R_first_order_sobol_dict, \n",
    "                                                                                           R_total_order_sobol_dict,\n",
    "                                                                                           filter_method=sobol_filter_method,\n",
    "                                                                                           divide_factor=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the dataframe that contains the number of neurons\n",
    "nb_important_vars = len(list(important_variables.keys()))\n",
    "determined_nb_important_vars = None\n",
    "if sobol_filter_method != 'top_portion' and sobol_filter_method != 'beside_end_portion':\n",
    "    determined_nb_important_vars = [[nb_vars, nb_important_vars, sobol_filter_method]]\n",
    "else:\n",
    "    determined_nb_important_vars = [[nb_vars, nb_important_vars, sobol_filter_method+'_'+sobol_divide_factor]]\n",
    "determined_nb_important_vars_df = pd.DataFrame(determined_nb_important_vars, columns=['nb_neurons', 'nb_important_neurons', 'sobol_filter_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the sorted total important variable(neuron) indices\n",
    "sorted_important_vars = sorted(list(important_variables))\n",
    "# Build the mapping dictionary to modify neuron indices\n",
    "important_var_map_dict = build_map_to_index_dict(important_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03603e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the important variables\n",
    "store_dict_as_json(path_join(output_path, trained_resnet_name+'_important_neurons.json'), important_variables)\n",
    "save_df_to_csv(path_join(output_path, trained_resnet_name+'_nb_important_neurons.csv'), determined_nb_important_vars_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d8835",
   "metadata": {},
   "source": [
    "### *Load the novelty OOD dataset and evaluate the activation levels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a7664-39bc-4493-a61f-907db98eccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the svhn dataset\n",
    "svhn_train_dataset, svhn_test_dataset = get_svhn_dataset_without_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c1bbc-0e24-478e-beb4-f56a5a7c8375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader building\n",
    "# svhn_train_loader = create_loader_from_torch_dataset(svhn_train_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)\n",
    "svhn_test_loader = create_loader_from_torch_dataset(svhn_test_dataset, batch_size=torch_batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5f373-3fcd-49f2-8a2e-d95677966ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dtd dataset\n",
    "dtd_train_dataset, dtd_test_dataset = get_dtd_dataset_resized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e0b41-f662-4d41-8603-2a0d7feb4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the train set to numpy array\n",
    "# no_divide_into_batch_dtd_train_loader = create_loader_from_torch_dataset(dtd_train_dataset, batch_size=len(dtd_train_dataset), shuffle=False, num_workers=0)\n",
    "# X_train_dtd = next(iter(no_divide_into_batch_dtd_train_loader))[0].numpy()\n",
    "# y_train_dtd = next(iter(no_divide_into_batch_dtd_train_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051feaf5-1952-4293-9983-c1a07eaa9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to numpy array\n",
    "no_divide_into_batch_dtd_test_loader = create_loader_from_torch_dataset(dtd_test_dataset, batch_size=len(dtd_test_dataset), shuffle=False, num_workers=0)\n",
    "X_test_dtd = next(iter(no_divide_into_batch_dtd_test_loader))[0].numpy()\n",
    "y_test_dtd = next(iter(no_divide_into_batch_dtd_test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745fb62-5352-411d-b32c-306207fadf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dtd loaders (using random original labels (because they are not important))\n",
    "# dtd_train_loader = create_dataloader(X_train_dtd, np.random.randint(0, data_set_infos['nb_classes'], y_train_dtd.shape[0]), \n",
    "#                                      batch_size=torch_batch_size, shuffle=False, type_conversion=True)\n",
    "dtd_test_loader = create_dataloader(X_test_dtd, np.random.randint(0, data_set_infos['nb_classes'], y_test_dtd.shape[0]), \n",
    "                                     batch_size=torch_batch_size, shuffle=False, type_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7abfd-233e-481e-b1ca-14ab4a0e11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the places365 dataset\n",
    "places_test_dataset = get_places_test_dataset_resized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54ecca-f9e5-4ea9-926a-3071575a1a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to numpy array\n",
    "no_divide_into_batch_places_test_loader = create_loader_from_torch_dataset(places_test_dataset, batch_size=len(places_test_dataset), shuffle=False, num_workers=0)\n",
    "X_test_places = next(iter(no_divide_into_batch_places_test_loader))[0].numpy()\n",
    "y_test_places = next(iter(no_divide_into_batch_places_test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc780c-2a12-479b-831e-147e0865e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the places365 loaders (using random original labels (because they are not important))\n",
    "places_test_loader = create_dataloader(X_test_places, np.random.randint(0, data_set_infos['nb_classes'], y_test_places.shape[0]), \n",
    "                                     batch_size=torch_batch_size, shuffle=False, type_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c8941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary that contains all the OOD dataset loaders\n",
    "novelty_loaders = {}\n",
    "novelty_loaders['svhn'] = svhn_test_loader\n",
    "novelty_loaders['dtd'] = dtd_test_loader\n",
    "novelty_loaders['places'] = places_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the OMS datasets for generating the normalized feature vectors\n",
    "novelty_actLevels = {}\n",
    "for ood_type in novelty_loaders:\n",
    "    novelty_actLevels[ood_type] = obtain_activation_levels(trained_resnet,\n",
    "                                                           novelty_loaders[ood_type], ood_type + ' test',\n",
    "                                                           with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c44c8b",
   "metadata": {},
   "source": [
    "### *Load the distribution shift OOD dataset (CIFAR-10-c) and evaluate the activation levels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the cifar10-c dataset\n",
    "# Get the content in the folder\n",
    "cifar10_c_data_files = [file for file in contents_of_folder(cifar10_c_path) if np_ext in file]\n",
    "# The number of images at each level\n",
    "nb_image_by_level = 10000\n",
    "# Load all the files\n",
    "load_cifar10_c = {}\n",
    "for file in cifar10_c_data_files:\n",
    "    # We always use the transformation_type as variable name for the ease of coding (even if it could be just \"labels\")  \n",
    "    file_type = str_first_part_split_from_r(file)\n",
    "    current_data = np.load(path_join(cifar10_c_path, file))\n",
    "    nb_batchs = current_data.shape[0] / nb_image_by_level\n",
    "    for index in range(int(nb_batchs)):\n",
    "        current_batch_data = current_data[index*nb_image_by_level:(index+1)*nb_image_by_level]\n",
    "        load_cifar10_c[file_type+'_s'+str(index+1)] = current_batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfa8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataloader and evaluate the activation levels\n",
    "distrib_shift_actLevels = {}\n",
    "for transformation_type in load_cifar10_c:\n",
    "    if 'labels' not in transformation_type:\n",
    "        # Get the severe level\n",
    "        severity = str_second_part_split_from_r(transformation_type, delimiter='_')\n",
    "        # Get the current image array\n",
    "        transformed_image_array = load_cifar10_c[transformation_type]\n",
    "        # Reshape the numpy array to satisfy pytorch model requirements     \n",
    "        pytorch_transformed_image_array = transformed_image_array.transpose(0,3,1,2)\n",
    "        # Normalize the pixel values to (0,1) range\n",
    "        pytorch_transformed_image_array = pytorch_transformed_image_array / image_max_pix_val\n",
    "        # Build the loader         \n",
    "        current_transformed_loader = create_dataloader(pytorch_transformed_image_array, load_cifar10_c['labels_'+severity],\n",
    "                                                                   torch_batch_size, shuffle=False, type_conversion=True)\n",
    "        # Evaluate the activation levels\n",
    "        distrib_shift_actLevels[transformation_type] = obtain_activation_levels(trained_resnet,\n",
    "                                           current_transformed_loader, transformation_type, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270610af",
   "metadata": {},
   "source": [
    "### *Load the adversarial OOD dataset and evaluate the activation levels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The registrered original dataset for the attacks\n",
    "original_X = None\n",
    "original_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a260fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the adversarial attacks\n",
    "# Find the loaded trained resnet attack path\n",
    "trained_resnet_attack_path = None\n",
    "for attack_folder in contents_of_folder(adv_attack_path):\n",
    "    if trained_resnet_name in attack_folder:\n",
    "        trained_resnet_attack_path = path_join(adv_attack_path, attack_folder)\n",
    "        break\n",
    "# Load all the attacks\n",
    "loaded_attacks = {}\n",
    "for attack_set in contents_of_folder(trained_resnet_attack_path):\n",
    "    current_attack_type = str_first_part_split_from_l(attack_set)\n",
    "    current_attack_set_path = path_join(trained_resnet_attack_path, attack_set)\n",
    "    if current_attack_type == 'original':\n",
    "        X_file_path = path_join(current_attack_set_path, 'X.npy')\n",
    "        y_file_path = path_join(current_attack_set_path, 'y.npy')\n",
    "        original_X = np.load(X_file_path)\n",
    "        original_y = np.load(y_file_path)\n",
    "    else:\n",
    "        attack_file_path = path_join(current_attack_set_path, contents_of_folder(current_attack_set_path)[0])\n",
    "        loaded_attacks[current_attack_type] = np.load(attack_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the attack loaders\n",
    "attack_loaders = {}\n",
    "for attack_type in loaded_attacks:\n",
    "    attack_loaders[attack_type] = create_dataloader(loaded_attacks[attack_type], original_y, torch_batch_size, shuffle=False, type_conversion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the attacks\n",
    "for attack_type in attack_loaders:\n",
    "    accuracy_eval(trained_resnet, attack_loaders[attack_type], set_name=attack_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the activation levels\n",
    "attack_actLevels = {}\n",
    "for attack_type in attack_loaders:\n",
    "    attack_actLevels[attack_type] = obtain_activation_levels(trained_resnet,\n",
    "                                       attack_loaders[attack_type], attack_type, with_predict_class=True, loss_type='cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b369a",
   "metadata": {},
   "source": [
    "### *Determine the correctly predicted training examples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed20d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean indicating the correctly predicted training examples\n",
    "correct_train_actLevels = build_correct_actLevels(train_actLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b8bc8",
   "metadata": {},
   "source": [
    "### *Prepare the normalized feature vectors with all neurons*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8beb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training set normalized feature vectors (only the correctly predicted examples)\n",
    "correct_train_zs = normalize_feature_vecs_knn(correct_train_actLevels, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training set normalized feature vectors\n",
    "train_zs = normalize_feature_vecs_knn(train_actLevels, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4af08-3ca9-41d3-9594-0bcb67a4cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the test set normalized feature vectors\n",
    "test_zs = normalize_feature_vecs_knn(test_actLevels, last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2001aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized feature vectors of the novelty ood set\n",
    "novelty_zs = {}\n",
    "for ood_type in novelty_actLevels:\n",
    "    novelty_zs[ood_type] = normalize_feature_vecs_knn(novelty_actLevels[ood_type], last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab956ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized feature vectors of the distribution shift ood set\n",
    "distrib_shift_zs = {}\n",
    "for transform_type in distrib_shift_actLevels:\n",
    "    distrib_shift_zs[transform_type] = normalize_feature_vecs_knn(distrib_shift_actLevels[transform_type], last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ade05",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_zs = {}\n",
    "for attack_type in attack_actLevels:\n",
    "    attack_zs[attack_type] = normalize_feature_vecs_knn(attack_actLevels[attack_type], last_hidden_layerId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c51ae",
   "metadata": {},
   "source": [
    "### *Prepare the Deep k-nearst neighbors OOD detection with all neurons*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c34847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the faiss train set index\n",
    "train_index = faiss.IndexFlatL2(correct_train_zs.shape[1])\n",
    "train_index.add(correct_train_zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e980b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the train set k-nearst neighbor scores\n",
    "correct_train_S = k_nearst_neighbor_scores(train_index, correct_train_zs, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887589a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean and std of the test scores\n",
    "threshold_S_mean, threshold_S_std = get_mean_std(correct_train_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcace6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the threshold\n",
    "threshold = threshold_S_mean - std_threshold_coeff * threshold_S_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33823685",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD evaluation result headers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_ood_eval_headers = ['set_name', 'nb_ood', 'nb_ind', 'ood_percent', 'ind_percent', 'acc_total', 'acc_ood', 'acc_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4674611",
   "metadata": {},
   "source": [
    "### *Evaluate the test accuracy after applying the OOD detection with all neurons on the original dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fcf048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ood results on the original datasets\n",
    "origin_ood_eval_results = []\n",
    "# The ood detection evaluation on the training set\n",
    "train_ood_eval_result = experim_ood_detection_knn(train_index, train_zs, train_actLevels, k, threshold, 'train')\n",
    "# Display jump line\n",
    "print()\n",
    "# The ood detection evaluation on the test set\n",
    "test_ood_eval_result = experim_ood_detection_knn(train_index, test_zs, test_actLevels, k, threshold, 'test')\n",
    "# Add the evaluation results\n",
    "origin_ood_eval_results.append(train_ood_eval_result)\n",
    "origin_ood_eval_results.append(test_ood_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fdafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'origin_ood_eval_results'+csv_ext), origin_ood_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57224d4f",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD detection on novelty dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69c0ff4-fc36-4657-96ae-7f205bfc1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ood detection evaluation\n",
    "novelty_ood_eval_results = []\n",
    "for ood_type in novelty_zs:\n",
    "    current_ood_eval_result = experim_ood_detection_knn(train_index, novelty_zs[ood_type],\n",
    "                                                        novelty_actLevels[ood_type], k, threshold, ood_type)\n",
    "    novelty_ood_eval_results.append(current_ood_eval_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distribution shift ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'novelty_ood_eval_results'+csv_ext), novelty_ood_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68ee99",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD detection on distribution shift dataset (CIFAR-10-c)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ood detection evaluation\n",
    "distrib_shift_eval_results = []\n",
    "for transform_type in distrib_shift_zs:\n",
    "    current_eval_result = experim_ood_detection_knn(train_index, distrib_shift_zs[transform_type],\n",
    "                                                    distrib_shift_actLevels[transform_type], k, threshold, transform_type)\n",
    "    distrib_shift_eval_results.append(current_eval_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distribution shift ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'distrib_shift_ood_eval_results'+csv_ext), distrib_shift_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12257c2f",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD detection on adversarial dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65229d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ood detection evaluation\n",
    "attack_eval_results = []\n",
    "for attack_type in attack_zs:\n",
    "    current_eval_result = experim_ood_detection_knn(train_index, attack_zs[attack_type],\n",
    "                                                    attack_actLevels[attack_type], k, threshold, attack_type)\n",
    "    attack_eval_results.append(current_eval_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f64332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distribution shift ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'adversarial_ood_eval_results'+csv_ext), attack_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e5fe7",
   "metadata": {},
   "source": [
    "### *Prepare the normalized feature vectors with only the significant neurons*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfae7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the normalized feature vectors according to the significant neurons of each class for the search index building\n",
    "correct_train_zs_sig = build_sig_zs(correct_train_actLevels, last_hidden_layerId, sorted_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the normalized feature vectors according to the significant neurons of each class for the training set\n",
    "train_zs_sig = build_sig_zs(train_actLevels, last_hidden_layerId, sorted_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e81c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the normalized feature vectors according to the significant neurons of each class for the test set\n",
    "test_zs_sig = build_sig_zs(test_actLevels, last_hidden_layerId, sorted_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized feature vectors of the novelty ood set using only the significant neurons\n",
    "novelty_zs_sig = {}\n",
    "for ood_type in novelty_actLevels:\n",
    "    novelty_zs_sig[ood_type] = build_sig_zs(novelty_actLevels[ood_type], last_hidden_layerId, sorted_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7372f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the normalized feature vectors from the significant neurons for the distribution shift ood set\n",
    "distrib_shift_zs_sig = {}\n",
    "for transform_type in distrib_shift_actLevels:\n",
    "    distrib_shift_zs_sig[transform_type] = build_sig_zs(distrib_shift_actLevels[transform_type], last_hidden_layerId, sorted_important_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b092f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the normalized feature vectors from the significant neurons for the adversarial ood set\n",
    "attack_zs_sig = {}\n",
    "for attack_type in attack_actLevels:\n",
    "    attack_zs_sig[attack_type] = build_sig_zs(attack_actLevels[attack_type], last_hidden_layerId, sorted_important_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e745ea",
   "metadata": {},
   "source": [
    "### *Prepare the Deep k-nearst neighbors OOD detection with the significant neurons*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the faiss train set index\n",
    "train_index_sig = faiss.IndexFlatL2(correct_train_zs_sig.shape[1])\n",
    "train_index_sig.add(correct_train_zs_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ce43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the scores over different classes\n",
    "correct_train_S_sig = k_nearst_neighbor_scores(train_index_sig, correct_train_zs_sig, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9698cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean and std of the train scores\n",
    "threshold_S_mean_sig, threshold_S_std_sig = get_mean_std(correct_train_S_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464aaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the threshold\n",
    "threshold_sig = threshold_S_mean_sig - std_threshold_coeff * threshold_S_std_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa8c52",
   "metadata": {},
   "source": [
    "### *Evaluate the test accuracy after applying the OOD detection with the significant neurons*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ood results on the original datasets\n",
    "sig_origin_ood_eval_results = []\n",
    "# The ood detection evaluation on the training set\n",
    "sig_train_ood_eval_result = experim_ood_detection_knn(train_index_sig, train_zs_sig, train_actLevels, k, threshold_sig, 'train')\n",
    "# Display jump line\n",
    "print()\n",
    "# The ood detection evaluation on the test set\n",
    "sig_test_ood_eval_result = experim_ood_detection_knn(train_index_sig, test_zs_sig, test_actLevels, k, threshold_sig, 'test')\n",
    "# Add the evaluation results\n",
    "sig_origin_ood_eval_results.append(sig_train_ood_eval_result)\n",
    "sig_origin_ood_eval_results.append(sig_test_ood_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'sig_origin_ood_eval_results'+csv_ext), sig_origin_ood_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a117a2-debe-48dd-883c-bf2a06d15ff1",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD detection with only the significant neurons on novelty dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77129b6-7788-4c70-bc5b-c8757cd7e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ood detection evaluation\n",
    "sig_novelty_ood_eval_results = []\n",
    "for ood_type in novelty_zs_sig:\n",
    "    current_eval_result = experim_ood_detection_knn(train_index_sig, novelty_zs_sig[ood_type],\n",
    "                                                    novelty_actLevels[ood_type], k, threshold_sig, ood_type)\n",
    "    sig_novelty_ood_eval_results.append(current_eval_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91983b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distribution shift ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'sig_novelty_ood_eval_results'+csv_ext), sig_novelty_ood_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210b44f",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD detection with only the significant neurons on distribution shift dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c55c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The ood detection evaluation\n",
    "sig_distrib_shift_eval_results = []\n",
    "for transform_type in distrib_shift_zs_sig:\n",
    "    current_eval_result = experim_ood_detection_knn(train_index_sig, distrib_shift_zs_sig[transform_type],\n",
    "                                                    distrib_shift_actLevels[transform_type], k, threshold_sig, transform_type)\n",
    "    sig_distrib_shift_eval_results.append(current_eval_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e643e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distribution shift ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'sig_distrib_shift_ood_eval_results'+csv_ext), sig_distrib_shift_eval_results, headers=knn_ood_eval_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27967eba",
   "metadata": {},
   "source": [
    "### *Deep k-nearst neighbors OOD detection with only the significant neurons on adversarial dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ood detection evaluation\n",
    "sig_attack_eval_results = []\n",
    "for attack_type in attack_zs_sig:\n",
    "    current_eval_result = experim_ood_detection_knn(train_index_sig, attack_zs_sig[attack_type],\n",
    "                                                    attack_actLevels[attack_type], k, threshold_sig, attack_type)\n",
    "    sig_attack_eval_results.append(current_eval_result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086eb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the distribution shift ood evaluation results\n",
    "save_list_to_csv(path_join(output_path, 'sig_adversarial_ood_eval_results'+csv_ext), sig_attack_eval_results, headers=knn_ood_eval_headers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
